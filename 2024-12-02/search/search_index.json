{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Parisod Lab Dry Guide \u00b6 The Dry Guide details the computational infrastructure and tasks used in the Parisod Lab. Looking for more resources? Check out our Parisod Lab code club GitHub repo for more coding tips and tricks! Other links: \u00b6 Parisod Lab website","title":"Home"},{"location":"#welcome_to_the_parisod_lab_dry_guide","text":"The Dry Guide details the computational infrastructure and tasks used in the Parisod Lab. Looking for more resources? Check out our Parisod Lab code club GitHub repo for more coding tips and tricks!","title":"Welcome to the Parisod Lab Dry Guide"},{"location":"#other_links","text":"Parisod Lab website","title":"Other links:"},{"location":"bash/","text":"Command line \u00b6 Command line Basic Commands More Advanced Good Guides grep awk Rearranging columns Filtering based on criteria bcftools Screen Bash is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important. Start with this introduction to bash . Also check out this cheatsheet Basic Commands \u00b6 You should familiarize yourself with the following commands. alias - create a shortcut for a command cat - concatenate files zcat - concatenate zipped files cd - change directories curl - download files echo - print strings export - Add a variable to the global environment so that they get passed on to child processes. grep - filter by pattern egrep - filter by regex rm - delete files sudo - run as an administrator sort - sorts files source - runs a file ssh - connect to servers which - locate files on your PATH uniq - get unique lines. File must be sorted. More Advanced \u00b6 You should learn these once you have the basics down. git - version control awk - file manipulation; Filtering; Rearranging columns sed - quick find/replace Good Guides \u00b6 Below are some good guides for various bash utilities. grep \u00b6 using grep with regular expressions another regex grep guide awk \u00b6 awk guide awk by example - hundreds of examples Rearranging columns \u00b6 cat example.tsv | awk -f OFS=\"\\t\" '{ print $2, $3, $1 }' The line above will print the second column, the third column and finally the first column. Filtering based on criteria \u00b6 Print only lines that start with a comment (#) character cat example.tsv awk '$0 ~ \"^#\" { print }' bcftools \u00b6 bcftools view bcftools view - view VCF bcftools view -h - view only header of VCF bcftools view -H - view VCF without header bcftools view -s CB4856,XZ1516,ECA701 - subset vcf for only these three samples bcftools view -S sample_file.txt - subset vcf for only samples listed in sample_file.txt bcftools view -r III:1-800000 - subset vcf for a region of interest can also just use -r III to get entire chromosome bcftools view -R regions.txt - subset vcf for a region(s) of interest in the regions.txt file bcftools query bcftools query -l - print out list of samples in vcf Print out contents of vcf in specified format (i.e. tsv): bcftools query -f '%CHROM\\t%POS\\t%REF\\t%ALT[\\t%SAMPLE=%GT]\\n' <vcf> > out.tsv Output of above line of code: bcftools query -i GT==\"alt\" - keep rows that include a tag (like a filter) bcftools query -e GT==\"ref\" - remove rows that include a tag Note bcftools query -i/e are not necessarily opposites. For example, if you have three genotype options (REF, ALT, or NA), including only ALT calls is different than exluding only REF calls... For more, check out the bcftools manual and this cheatsheet Screen \u00b6 Screen can be used to run things in the background. It is extremely useful if you need to run things on quest without worry that they will be terminated if you log out or get kicked off. This is essential when running nextflow because pipelines can sometimes run for many hours and its likely you will be kicked off in that time or lose your connection. Screen basics","title":"Command Line"},{"location":"bash/#command_line","text":"Command line Basic Commands More Advanced Good Guides grep awk Rearranging columns Filtering based on criteria bcftools Screen Bash is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important. Start with this introduction to bash . Also check out this cheatsheet","title":"Command line"},{"location":"bash/#basic_commands","text":"You should familiarize yourself with the following commands. alias - create a shortcut for a command cat - concatenate files zcat - concatenate zipped files cd - change directories curl - download files echo - print strings export - Add a variable to the global environment so that they get passed on to child processes. grep - filter by pattern egrep - filter by regex rm - delete files sudo - run as an administrator sort - sorts files source - runs a file ssh - connect to servers which - locate files on your PATH uniq - get unique lines. File must be sorted.","title":"Basic Commands"},{"location":"bash/#more_advanced","text":"You should learn these once you have the basics down. git - version control awk - file manipulation; Filtering; Rearranging columns sed - quick find/replace","title":"More Advanced"},{"location":"bash/#good_guides","text":"Below are some good guides for various bash utilities.","title":"Good Guides"},{"location":"bash/#grep","text":"using grep with regular expressions another regex grep guide","title":"grep"},{"location":"bash/#awk","text":"awk guide awk by example - hundreds of examples","title":"awk"},{"location":"bash/#rearranging_columns","text":"cat example.tsv | awk -f OFS=\"\\t\" '{ print $2, $3, $1 }' The line above will print the second column, the third column and finally the first column.","title":"Rearranging columns"},{"location":"bash/#filtering_based_on_criteria","text":"Print only lines that start with a comment (#) character cat example.tsv awk '$0 ~ \"^#\" { print }'","title":"Filtering based on criteria"},{"location":"bash/#bcftools","text":"bcftools view bcftools view - view VCF bcftools view -h - view only header of VCF bcftools view -H - view VCF without header bcftools view -s CB4856,XZ1516,ECA701 - subset vcf for only these three samples bcftools view -S sample_file.txt - subset vcf for only samples listed in sample_file.txt bcftools view -r III:1-800000 - subset vcf for a region of interest can also just use -r III to get entire chromosome bcftools view -R regions.txt - subset vcf for a region(s) of interest in the regions.txt file bcftools query bcftools query -l - print out list of samples in vcf Print out contents of vcf in specified format (i.e. tsv): bcftools query -f '%CHROM\\t%POS\\t%REF\\t%ALT[\\t%SAMPLE=%GT]\\n' <vcf> > out.tsv Output of above line of code: bcftools query -i GT==\"alt\" - keep rows that include a tag (like a filter) bcftools query -e GT==\"ref\" - remove rows that include a tag Note bcftools query -i/e are not necessarily opposites. For example, if you have three genotype options (REF, ALT, or NA), including only ALT calls is different than exluding only REF calls... For more, check out the bcftools manual and this cheatsheet","title":"bcftools"},{"location":"bash/#screen","text":"Screen can be used to run things in the background. It is extremely useful if you need to run things on quest without worry that they will be terminated if you log out or get kicked off. This is essential when running nextflow because pipelines can sometimes run for many hours and its likely you will be kicked off in that time or lose your connection. Screen basics","title":"Screen"},{"location":"best_practices/","text":"Parisod Lab Coding Best Practices \u00b6 These best practices are just a few of the important coding tips and tricks for reproducible research. If you have more ideas, contact Rimjhim! General \u00b6 You should be doing most (if not all) of your analyses in LabDrives/YourProjectName (except for IBU, see below) This is (1) to make sure the data is backed up/saved with version history and (2) to allow other lab members to access your code/scripts when necessary Do NOT use spaces in names of files or folders. Try not to use spaces in column names too (although sometimes it is necessary for a final table output) Computers often have a hard time reading spaces and code used to ignore spaces can vary from program to program Instead, you can use _ or . or - or capitalization ( fileName.txt ) NEVER replace raw data!!!!! You should save your raw data in the rawest format, and submitted to ENA as soon as possible. Write a script to analyze it, then if you wish, save the processed data for further use. This is important because it always allows you to go back to the original raw data in case something happens Some suggested project folder structure might look like something below: Include a README.md (or README.txt ) file in each project folder to explain where the data and scripts can be found for certain analyses. Trust me, after a few years you will definitely forget\u2026 And don\u2019t forget to update the README regularly, an old README doesn\u2019t do anyone good! Either use full path names in scripts or be explicit about where the working directory is This is important to allow other people to run your code (or might even be helpful for you if you ever reorganize folders one day) Date your files, especially when you update an existing file. Write dates in YYYYMMDD format As much as possible, ensure that your processed data is \u201ctidy\u201d (see below). This doesn\u2019t work for all complex data types, but it should be a general norm to follow. Each variable must have its own column Each observation must have its own row Each value must have its own cell No color or highlighting No empty cells (fill with NA if necessary) Save data as plain text files ( .csv , .tsv , .txt etc. -- NOT .xls !!!) R \u00b6 ALWAYS use namespaces before functions from packages (i.e. dplyr::filter() instead of filter() ) This includes ggplot2 and especially dplyr !!! Some packages have functions with the same name, so adding a namespace is crucial for reproducibility. Also, this helps other people read your code by knowing which functions came from which packages When piping with Tidyverse ( %>% ), press <Enter> to go to the next line after a pipe This makes your code more readable In fact, general practices state no more than 80-100 characters per line of code EVER to increase readability IBU \u00b6 You should be doing most (if not all) of your analyses in /data/projects/<project_name>/<user_id> or /data/users/<user_id> (not your home directory (i.e. /home/<user_id> )) Important Main exception: Snakemake and Nextflow temporary working directories should NOT be on /data (it will fill us up!) but rather in the scratch space (files get automatically deleted here every 30 days). A correctly designed nextflow.config file will take care of this. For Snakemake, you can set the --directory flag to /scratch/<user_id>/<project_name> (or similar) Python \u00b6 [ Needs filling in from someone who uses python :) ]","title":"Best Practices"},{"location":"best_practices/#parisod_lab_coding_best_practices","text":"These best practices are just a few of the important coding tips and tricks for reproducible research. If you have more ideas, contact Rimjhim!","title":"Parisod Lab Coding Best Practices"},{"location":"best_practices/#general","text":"You should be doing most (if not all) of your analyses in LabDrives/YourProjectName (except for IBU, see below) This is (1) to make sure the data is backed up/saved with version history and (2) to allow other lab members to access your code/scripts when necessary Do NOT use spaces in names of files or folders. Try not to use spaces in column names too (although sometimes it is necessary for a final table output) Computers often have a hard time reading spaces and code used to ignore spaces can vary from program to program Instead, you can use _ or . or - or capitalization ( fileName.txt ) NEVER replace raw data!!!!! You should save your raw data in the rawest format, and submitted to ENA as soon as possible. Write a script to analyze it, then if you wish, save the processed data for further use. This is important because it always allows you to go back to the original raw data in case something happens Some suggested project folder structure might look like something below: Include a README.md (or README.txt ) file in each project folder to explain where the data and scripts can be found for certain analyses. Trust me, after a few years you will definitely forget\u2026 And don\u2019t forget to update the README regularly, an old README doesn\u2019t do anyone good! Either use full path names in scripts or be explicit about where the working directory is This is important to allow other people to run your code (or might even be helpful for you if you ever reorganize folders one day) Date your files, especially when you update an existing file. Write dates in YYYYMMDD format As much as possible, ensure that your processed data is \u201ctidy\u201d (see below). This doesn\u2019t work for all complex data types, but it should be a general norm to follow. Each variable must have its own column Each observation must have its own row Each value must have its own cell No color or highlighting No empty cells (fill with NA if necessary) Save data as plain text files ( .csv , .tsv , .txt etc. -- NOT .xls !!!)","title":"General"},{"location":"best_practices/#r","text":"ALWAYS use namespaces before functions from packages (i.e. dplyr::filter() instead of filter() ) This includes ggplot2 and especially dplyr !!! Some packages have functions with the same name, so adding a namespace is crucial for reproducibility. Also, this helps other people read your code by knowing which functions came from which packages When piping with Tidyverse ( %>% ), press <Enter> to go to the next line after a pipe This makes your code more readable In fact, general practices state no more than 80-100 characters per line of code EVER to increase readability","title":"R"},{"location":"best_practices/#ibu","text":"You should be doing most (if not all) of your analyses in /data/projects/<project_name>/<user_id> or /data/users/<user_id> (not your home directory (i.e. /home/<user_id> )) Important Main exception: Snakemake and Nextflow temporary working directories should NOT be on /data (it will fill us up!) but rather in the scratch space (files get automatically deleted here every 30 days). A correctly designed nextflow.config file will take care of this. For Snakemake, you can set the --directory flag to /scratch/<user_id>/<project_name> (or similar)","title":"IBU"},{"location":"best_practices/#python","text":"[ Needs filling in from someone who uses python :) ]","title":"Python"},{"location":"ena/","text":"Uploading sequence data to ENA \u00b6 For each project, it is important to also upload the FASTQ files and other raw sequncing files to ENA or SRA (See: sra.md ) as soon as the sequencing data arrives. If the study ID already exists, you can create a new sample ID link to the existing study ID . If there is no previous study ID, you can create a new study ID and add all relevant data. See below for more instructions. ENA submission \u00b6 Begin submission In the SRA submission portal , click the button for \"New submission\" and follow the prompts. Remember to add the previous study ID if applicable to link this submission to previous submissions. Select \"Model organism or animal\" for biosample type, select \"upload file using excel\" and download the template Create biosample sheet (an example can be found below) An easy starting point here is the sample sheet used for alignment-nf . You will keep the id as the sample name (unique identifier) and strain (note strain also needs to be unique - if there are multiple library preps for the same strain, please append \"-2\" etc. to the strain) To these two columns, you can add bioproject, organism, developmental stage, sex, and tissue (same across all strains) Finally, join this data with the WI species master sheet to get collected by, collection date, and latitude/longitude. - Note: latitude/longitude need to be converted into one shared column in the format \"34.89 S 56.15 W\" (+ refers to North and East) Copy the data into the relevent columns in the template , save, and upload to the submission portal Create SRA metadata sheet Again, select \"upload a file using excel\" and download the template An easy starting point here is, again, the sample sheet used for alignment-nf . You will keep the id as the sample name and the lb as library id. You will also keep fq1 and fq2 for filename and filename2. You will then add the rest of the columns as shown below. Note: the formatting is very specific for this sheet. The title can be found on the bioproject page. The instrument_model can be found by using the \"sequencing_folder\" (not shown, but part of the original sample sheet) and looking up the instrument that folder was run on in the Sequencing Runs google sheet ( here ) Copy and paste the rows from this file into the template to check for correct formatting. Then save the tab as a tsv and upload to the submission portal. Pre-upload FASTQ files using FTP Create a list of files to upload to the FTP server by combining the filename1 and filename2 from the SRA metadata sheet (above). Begin submission by creating an NCBI account (or signing in -- personal account). Then follow the link to the SRA submission portal Follow the instructions under the \"FTP upload\": # establish FTP connection from terminal (on QUEST!) # ftp <address> ftp ftp-private.ncbi.nlm.nih.gov # navigate to your account folder (i.e.) cd uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4 # create new folder for submission (i.e.) mkdir 20210121_submission # exit FTP connection exit # back on quest, run the following line to transfer every file with path listed in \"files_to_upload.tsv\" to that folder # make sure to change your upload folder and files to upload module load parallel parallel --verbose lftp -e \\\"put -O /uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4/20210121_submission {}\\; bye\\\" -u subftp,w4pYB9VQ ftp-private.ncbi.nlm.nih.gov < files_to_upload.tsv Note: it is important that this step is completely finished before you complete your SRA submission Complete submission When finished, in the SRA portal you will be asked to select which folder you want to pull files from Review and submit! If there are any issues they will let you know. List of current bioprojects associated with the Andersen Lab \u00b6 C. elegans WI genome FASTQ - PRJNA549503 (link here )","title":"Uploading sequence data to ENA"},{"location":"ena/#uploading_sequence_data_to_ena","text":"For each project, it is important to also upload the FASTQ files and other raw sequncing files to ENA or SRA (See: sra.md ) as soon as the sequencing data arrives. If the study ID already exists, you can create a new sample ID link to the existing study ID . If there is no previous study ID, you can create a new study ID and add all relevant data. See below for more instructions.","title":"Uploading sequence data to ENA"},{"location":"ena/#ena_submission","text":"Begin submission In the SRA submission portal , click the button for \"New submission\" and follow the prompts. Remember to add the previous study ID if applicable to link this submission to previous submissions. Select \"Model organism or animal\" for biosample type, select \"upload file using excel\" and download the template Create biosample sheet (an example can be found below) An easy starting point here is the sample sheet used for alignment-nf . You will keep the id as the sample name (unique identifier) and strain (note strain also needs to be unique - if there are multiple library preps for the same strain, please append \"-2\" etc. to the strain) To these two columns, you can add bioproject, organism, developmental stage, sex, and tissue (same across all strains) Finally, join this data with the WI species master sheet to get collected by, collection date, and latitude/longitude. - Note: latitude/longitude need to be converted into one shared column in the format \"34.89 S 56.15 W\" (+ refers to North and East) Copy the data into the relevent columns in the template , save, and upload to the submission portal Create SRA metadata sheet Again, select \"upload a file using excel\" and download the template An easy starting point here is, again, the sample sheet used for alignment-nf . You will keep the id as the sample name and the lb as library id. You will also keep fq1 and fq2 for filename and filename2. You will then add the rest of the columns as shown below. Note: the formatting is very specific for this sheet. The title can be found on the bioproject page. The instrument_model can be found by using the \"sequencing_folder\" (not shown, but part of the original sample sheet) and looking up the instrument that folder was run on in the Sequencing Runs google sheet ( here ) Copy and paste the rows from this file into the template to check for correct formatting. Then save the tab as a tsv and upload to the submission portal. Pre-upload FASTQ files using FTP Create a list of files to upload to the FTP server by combining the filename1 and filename2 from the SRA metadata sheet (above). Begin submission by creating an NCBI account (or signing in -- personal account). Then follow the link to the SRA submission portal Follow the instructions under the \"FTP upload\": # establish FTP connection from terminal (on QUEST!) # ftp <address> ftp ftp-private.ncbi.nlm.nih.gov # navigate to your account folder (i.e.) cd uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4 # create new folder for submission (i.e.) mkdir 20210121_submission # exit FTP connection exit # back on quest, run the following line to transfer every file with path listed in \"files_to_upload.tsv\" to that folder # make sure to change your upload folder and files to upload module load parallel parallel --verbose lftp -e \\\"put -O /uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4/20210121_submission {}\\; bye\\\" -u subftp,w4pYB9VQ ftp-private.ncbi.nlm.nih.gov < files_to_upload.tsv Note: it is important that this step is completely finished before you complete your SRA submission Complete submission When finished, in the SRA portal you will be asked to select which folder you want to pull files from Review and submit! If there are any issues they will let you know.","title":"ENA submission"},{"location":"ena/#list_of_current_bioprojects_associated_with_the_andersen_lab","text":"C. elegans WI genome FASTQ - PRJNA549503 (link here )","title":"List of current bioprojects associated with the Andersen Lab"},{"location":"github/","text":"Source Control with Git and VSCode \u00b6 Source Control with Git and VSCode Introduction Using Git and Github The GitHub Flow 1. Clone/pull 2. Branch 3. Edit 4. Commit 5. Push 6. Pull request 7. Inspect 8. Merge Command Line git commands GitHub Flow Best Practices Using Git with VSCode Resources Introduction \u00b6 Git is a version control software package similar to traccked changes and saving documents as \"final1.pdf\" and \"final2.pdf\" Github is a 3rd party web-based graphical interface that has a copy of the project that you and/or other people can push and pull from to work on the same code simultaneously. Keep in mind, its not like google docs, it doesn\u2019t update automatically, requires you to push changes and pull changes to the new computer. Note You must install git to use and create an account on Github. Check out this intro guide here Using Git and Github \u00b6 The Parisod Lab Github can be found here . As of the writing of this page, we have 165 \"repositories\" (or projects). Notice some projects are workflow like snakeGATK4_v2 and others are shared projects ( codeclub ) or manuscripts ( mbe_manuscript_2024 ). Anything can be a repo! There are two main ways to use Git: on the command line (aka Terminal on Macs) or with a GUI (graphical user interface). Both are good and neither are \"wrong\". For new users, it is usually easier to start with a GUI like the Source control extension in VSCode. However, only a few basic commands are really necessary to get started using git on the command line, so don't be nervous! Important When you are working on a collaborative project with multiple people, you should always be working in a branch. This is a copy of the project that you can make changes to without affecting the main project. The GitHub Flow \u00b6 There are several different Git branching strategies, but the most popular for our lab is the \"GitHub Flow\". This 8 step process can help keep our pipelines flowing, functional, and organized. The following 8 steps can be done on the command line or with a GUI. Below I will show the basic git commands for managing a repo on the command line. Note When you are maintaining a project repo that only you are updating, it is less important to follow the GitHub Flow with creating short-lived branches. However, if you are developing/maintaining code that other people will use and/or working collaboratively this is an essential skill to master. 1. Clone/pull \u00b6 # Cloning - new repo cd < directory you want repo stored > https://github.com/parisodlab/snakeGATK4_v2 cd snakeGATK4_v2 # pulling - already cloned repo you want to get newest version of cd < directory of repo > git pull 2. Branch \u00b6 # create a new branch AND move to it git checkout -b <branch_name> # list all available branches git branch # move to a branch git checkout < name of branch > 3. Edit \u00b6 No code here... make any edits to the repo. 4. Commit \u00b6 # first step - add changed files to staging area git add <changed file> # OR add ALL files to staging area git add . # commit files in staging area git commit -m \"<some message about what changes you made>\" 5. Push \u00b6 # push changes to remote git push # when it is your first time pushing a new branch, it might prompt you to set an upstream branch: git push --set-upstream origin new_branch 6. Pull request \u00b6 I generally like to do this step online at github.com because I think it is useful to visually see the changes I made go to the repo site click the green \"compare and pull request\" button check the branches are right at the top: which branch is merging into which branch optional: assing reviewer and/or assignees on the right hand side. This is often useful when coding collaboratively update the title/comment for the pull request to let yourself and others know what changes were made and why 7. Inspect \u00b6 I generally like to do this step online at github.com because I think it is useful to visually see the changes I made. If you scroll down you should be able to see which files were changed and what exact changes were made. If there are merge conflicts, github will walk you through fixing them. 8. Merge \u00b6 When you are satisfied with your merge, click the green \"merge pull request\" button. Also make sure the delete the old branch when you are done as part of keeping the repo clean and clutter-free Note Good practice is to make a new branch to implement a new feature, then delete the branch once it has been merged. To start a new feature, open a NEW branch. Not as important on self-projects, but very important for collaboration Command Line git commands \u00b6 Basic git clone - clone remote repository git pull - pull most recent version from remote git add - add local files to be staged for remote git commit - stage/commit local changes git push - push local commits to remote Intermediate git branch - list all available branches git checkout - move to new branch git status - checks which branch you are on and if you have any unsaved changes git log - shows log of previous commits on current branch git diff - shows details of changes made For more, check out this tutorial, and others. GitHub Flow Best Practices \u00b6 Any code in the main branch should be deployable Create new descriptively-named branches off the main branch for new work such as feature/add-new-plot Commit new work to your local branches and regularly push work to the remote To request feedback or help, or when you think your work is ready to merge into the main branch, open a pull request After your work or feature has been reviewed and approved, it can be merged into the main branch Delete stale branches! Once your work has been merged into the main branch, it should be deployed immediately Note GitHub Flow is not the only branching strategy out there! This was a great article about the three most common strategies with pros and cons for why you might use each one. I challenge you to think aobut which strategy might be best for our lab moving forward and let's start a discussion about it! Using Git with VSCode \u00b6 VSCode has a built-in source control extension that makes it easy to use git commands without the command line. This is a great way to get started with git and learn the basics of version control. Open the source control tab on the left side of the screen Click the \"+\" to stage changes Add a commit message and click the checkmark to commit changes Click the three dots to push changes to the remote repository Click the three dots to pull changes from the remote repository Click the three dots to create a new branch Resources \u00b6 This blog on the differences between git and github \"Git started\" using Git on the command line here Overview of top Git GUI from 2021 here Great intro video to the GitHub Flow HIGHLY RECOMMENDED introduction tutorial to GitHub Flow Amazing article on different git branch strategies here","title":"Source Control with Git and VScode"},{"location":"github/#source_control_with_git_and_vscode","text":"Source Control with Git and VSCode Introduction Using Git and Github The GitHub Flow 1. Clone/pull 2. Branch 3. Edit 4. Commit 5. Push 6. Pull request 7. Inspect 8. Merge Command Line git commands GitHub Flow Best Practices Using Git with VSCode Resources","title":"Source Control with Git and VSCode"},{"location":"github/#introduction","text":"Git is a version control software package similar to traccked changes and saving documents as \"final1.pdf\" and \"final2.pdf\" Github is a 3rd party web-based graphical interface that has a copy of the project that you and/or other people can push and pull from to work on the same code simultaneously. Keep in mind, its not like google docs, it doesn\u2019t update automatically, requires you to push changes and pull changes to the new computer. Note You must install git to use and create an account on Github. Check out this intro guide here","title":"Introduction"},{"location":"github/#using_git_and_github","text":"The Parisod Lab Github can be found here . As of the writing of this page, we have 165 \"repositories\" (or projects). Notice some projects are workflow like snakeGATK4_v2 and others are shared projects ( codeclub ) or manuscripts ( mbe_manuscript_2024 ). Anything can be a repo! There are two main ways to use Git: on the command line (aka Terminal on Macs) or with a GUI (graphical user interface). Both are good and neither are \"wrong\". For new users, it is usually easier to start with a GUI like the Source control extension in VSCode. However, only a few basic commands are really necessary to get started using git on the command line, so don't be nervous! Important When you are working on a collaborative project with multiple people, you should always be working in a branch. This is a copy of the project that you can make changes to without affecting the main project.","title":"Using Git and Github"},{"location":"github/#the_github_flow","text":"There are several different Git branching strategies, but the most popular for our lab is the \"GitHub Flow\". This 8 step process can help keep our pipelines flowing, functional, and organized. The following 8 steps can be done on the command line or with a GUI. Below I will show the basic git commands for managing a repo on the command line. Note When you are maintaining a project repo that only you are updating, it is less important to follow the GitHub Flow with creating short-lived branches. However, if you are developing/maintaining code that other people will use and/or working collaboratively this is an essential skill to master.","title":"The GitHub Flow"},{"location":"github/#1_clonepull","text":"# Cloning - new repo cd < directory you want repo stored > https://github.com/parisodlab/snakeGATK4_v2 cd snakeGATK4_v2 # pulling - already cloned repo you want to get newest version of cd < directory of repo > git pull","title":"1. Clone/pull"},{"location":"github/#2_branch","text":"# create a new branch AND move to it git checkout -b <branch_name> # list all available branches git branch # move to a branch git checkout < name of branch >","title":"2. Branch"},{"location":"github/#3_edit","text":"No code here... make any edits to the repo.","title":"3. Edit"},{"location":"github/#4_commit","text":"# first step - add changed files to staging area git add <changed file> # OR add ALL files to staging area git add . # commit files in staging area git commit -m \"<some message about what changes you made>\"","title":"4. Commit"},{"location":"github/#5_push","text":"# push changes to remote git push # when it is your first time pushing a new branch, it might prompt you to set an upstream branch: git push --set-upstream origin new_branch","title":"5. Push"},{"location":"github/#6_pull_request","text":"I generally like to do this step online at github.com because I think it is useful to visually see the changes I made go to the repo site click the green \"compare and pull request\" button check the branches are right at the top: which branch is merging into which branch optional: assing reviewer and/or assignees on the right hand side. This is often useful when coding collaboratively update the title/comment for the pull request to let yourself and others know what changes were made and why","title":"6. Pull request"},{"location":"github/#7_inspect","text":"I generally like to do this step online at github.com because I think it is useful to visually see the changes I made. If you scroll down you should be able to see which files were changed and what exact changes were made. If there are merge conflicts, github will walk you through fixing them.","title":"7. Inspect"},{"location":"github/#8_merge","text":"When you are satisfied with your merge, click the green \"merge pull request\" button. Also make sure the delete the old branch when you are done as part of keeping the repo clean and clutter-free Note Good practice is to make a new branch to implement a new feature, then delete the branch once it has been merged. To start a new feature, open a NEW branch. Not as important on self-projects, but very important for collaboration","title":"8. Merge"},{"location":"github/#command_line_git_commands","text":"Basic git clone - clone remote repository git pull - pull most recent version from remote git add - add local files to be staged for remote git commit - stage/commit local changes git push - push local commits to remote Intermediate git branch - list all available branches git checkout - move to new branch git status - checks which branch you are on and if you have any unsaved changes git log - shows log of previous commits on current branch git diff - shows details of changes made For more, check out this tutorial, and others.","title":"Command Line git commands"},{"location":"github/#github_flow_best_practices","text":"Any code in the main branch should be deployable Create new descriptively-named branches off the main branch for new work such as feature/add-new-plot Commit new work to your local branches and regularly push work to the remote To request feedback or help, or when you think your work is ready to merge into the main branch, open a pull request After your work or feature has been reviewed and approved, it can be merged into the main branch Delete stale branches! Once your work has been merged into the main branch, it should be deployed immediately Note GitHub Flow is not the only branching strategy out there! This was a great article about the three most common strategies with pros and cons for why you might use each one. I challenge you to think aobut which strategy might be best for our lab moving forward and let's start a discussion about it!","title":"GitHub Flow Best Practices"},{"location":"github/#using_git_with_vscode","text":"VSCode has a built-in source control extension that makes it easy to use git commands without the command line. This is a great way to get started with git and learn the basics of version control. Open the source control tab on the left side of the screen Click the \"+\" to stage changes Add a commit message and click the checkmark to commit changes Click the three dots to push changes to the remote repository Click the three dots to pull changes from the remote repository Click the three dots to create a new branch","title":"Using Git with VSCode"},{"location":"github/#resources","text":"This blog on the differences between git and github \"Git started\" using Git on the command line here Overview of top Git GUI from 2021 here Great intro video to the GitHub Flow HIGHLY RECOMMENDED introduction tutorial to GitHub Flow Amazing article on different git branch strategies here","title":"Resources"},{"location":"ibu-conda/","text":"Using conda on IBU \u00b6 Using conda on IBU Why Conda Setting up Conda on IBU Using Conda Running Snakemake with conda Notes on conda versions on IBU A faster alternative to Conda Why Conda \u00b6 Computational Reproducibility ensures that an analysis can be exactly replicated. To achieve this in computational research, it is essential to track the code, software, and data used. Code is managed using git and GitHub (refer to the Github page for assistance). Typically, the initial data (often FASTQs) remains unchanged, so tracking data changes is unnecessary. New analyses are conducted when additional data is introduced. Software, however, is updated frequently, and the same version of software may not be the latest in the future. To track software, package and environment managers such as Conda and Singularity/Docker are very useful. Setting up Conda on IBU \u00b6 Anaconda is a Python distribution that contains many packages including conda. Miniconda is a more compact version of Anaconda that also includes conda. So in order to install conda, we usually either install Miniconda or Anaconda. On IBU, Anaconda is already installed. However there are many versions of Anaconda, each can have a different version of Python and Conda. The current environments at IBU is available using module load Anaconda3/2022.05 (See Notes below for more info) You can check the availability in future using the command module avail |& grep -i conda . In your home directory ~/ , create a file called .condarc and put the following lines into it. It sets the channel priority when conda searches for packages. If possible, in one environment it is good to use packages from the same channel. channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /data/users/<user_id>/conda_envs/ Using Conda \u00b6 Conda Documentation Conda Cheatsheet After loading the Anaconda module, one can create an environment and install packages into that environment: # Create conda environment named \"name_of_env\" conda create --name name_of_env # Create conda environment in a specific folder conda create -p /data/users/<user_id>/conda_envs/test # activate conda environment # for some `conda activate` works and for others `source activate` conda activate test source activate test # install package into conda environment conda install bcftools When looking to install a package, one resource to check out is anaconda.org , search for your package and run the install command listed on the page. Note Remember to keep in mind the different versions of software/packages. You could have bcftools-v1.10 or bcftools-v.1.12, so make sure you install the correct one! Important You can also install R packages with conda, however conda and R don't always work well together. Check out our quick fix here Running Snakemake with conda \u00b6 When running Snakemake, Conda environments should be stored in a subfolder workflow/envs (make sure to keep them as finegrained as possible to improve transparency and maintainability) (check out the documentation ): Conda within a rule: rule NAME: input: \"table.txt\" output: \"plots/myplot.pdf\" conda: \"envs/ggplot.yaml\" script: \"scripts/plot-stuff.R\" Conda environment snake_env.yaml for the entire workflow: (with snakemake for execution working great in 2024) name: snake_env channels: - conda-forge - bioconda - defaults - R dependencies: - r-base=4.3.2 - snakemake=8.16.0 - snakemake-minimal=8.16.0 - snakemake-wrapper-utils=0.1.3 - pigz=2.3.4 - toposort=1.10 - python>=3.10 - pip=23.1.2 # bioconda channel installs - fastqc - trimmomatic=0.36 - multiqc - qualimap - pip: - snakemake-executor-plugin-cluster-generic Notes on conda versions on IBU \u00b6 As of 2024, conda environments generated with module load anaconda3/2022.05 from IBU loads Python version 3.9.12 and conda 4.12.0. Once you activate an environment with conda activate env_name or source activate env_name , the default conda usually get re-directed to the conda that were originally used to create the environment. This is good because it helps ensure that all packages in the same environment uses the same version of conda. One can go to cd ~/.conda/env_name/bin and readlink -f conda or readlink -f activate to see which version of conda is used by this environment. This is exactly how Snakemake determines which conda to use when using an existing conda environment. A faster alternative to Conda \u00b6 In recent years, a 'drop-in' replacement for Conda, called Mamba, was released. Mamba is faster at resolving dependencies, which improves wait times for creating and activating environments. Conda and Mamba seem to be forward and backwards compatible, and they share the same command structure. For example, conda create and mamba create do the same task and share the same parameters. If you prefer to use Mamba, you can install it in your home directory by running these commands: cd ~ wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh chmod +x ./Mambaforge-Linux-x86_64.sh ./Mambaforge-Linux-x86_64.sh After completing the installation, the mamba binary should be added to $PATH , and you should be able to create, configure, and activate environments using the same commands as conda (FYI the conda binary will still be available to be called from $PATH by default). You will immediately notice a difference in speed when using mamba. Test it by loading an environment: mamba activate /data/users/<user_id>/conda_envs/<env_name>","title":"Conda"},{"location":"ibu-conda/#using_conda_on_ibu","text":"Using conda on IBU Why Conda Setting up Conda on IBU Using Conda Running Snakemake with conda Notes on conda versions on IBU A faster alternative to Conda","title":"Using conda on IBU"},{"location":"ibu-conda/#why_conda","text":"Computational Reproducibility ensures that an analysis can be exactly replicated. To achieve this in computational research, it is essential to track the code, software, and data used. Code is managed using git and GitHub (refer to the Github page for assistance). Typically, the initial data (often FASTQs) remains unchanged, so tracking data changes is unnecessary. New analyses are conducted when additional data is introduced. Software, however, is updated frequently, and the same version of software may not be the latest in the future. To track software, package and environment managers such as Conda and Singularity/Docker are very useful.","title":"Why Conda"},{"location":"ibu-conda/#setting_up_conda_on_ibu","text":"Anaconda is a Python distribution that contains many packages including conda. Miniconda is a more compact version of Anaconda that also includes conda. So in order to install conda, we usually either install Miniconda or Anaconda. On IBU, Anaconda is already installed. However there are many versions of Anaconda, each can have a different version of Python and Conda. The current environments at IBU is available using module load Anaconda3/2022.05 (See Notes below for more info) You can check the availability in future using the command module avail |& grep -i conda . In your home directory ~/ , create a file called .condarc and put the following lines into it. It sets the channel priority when conda searches for packages. If possible, in one environment it is good to use packages from the same channel. channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /data/users/<user_id>/conda_envs/","title":"Setting up Conda on IBU"},{"location":"ibu-conda/#using_conda","text":"Conda Documentation Conda Cheatsheet After loading the Anaconda module, one can create an environment and install packages into that environment: # Create conda environment named \"name_of_env\" conda create --name name_of_env # Create conda environment in a specific folder conda create -p /data/users/<user_id>/conda_envs/test # activate conda environment # for some `conda activate` works and for others `source activate` conda activate test source activate test # install package into conda environment conda install bcftools When looking to install a package, one resource to check out is anaconda.org , search for your package and run the install command listed on the page. Note Remember to keep in mind the different versions of software/packages. You could have bcftools-v1.10 or bcftools-v.1.12, so make sure you install the correct one! Important You can also install R packages with conda, however conda and R don't always work well together. Check out our quick fix here","title":"Using Conda"},{"location":"ibu-conda/#running_snakemake_with_conda","text":"When running Snakemake, Conda environments should be stored in a subfolder workflow/envs (make sure to keep them as finegrained as possible to improve transparency and maintainability) (check out the documentation ): Conda within a rule: rule NAME: input: \"table.txt\" output: \"plots/myplot.pdf\" conda: \"envs/ggplot.yaml\" script: \"scripts/plot-stuff.R\" Conda environment snake_env.yaml for the entire workflow: (with snakemake for execution working great in 2024) name: snake_env channels: - conda-forge - bioconda - defaults - R dependencies: - r-base=4.3.2 - snakemake=8.16.0 - snakemake-minimal=8.16.0 - snakemake-wrapper-utils=0.1.3 - pigz=2.3.4 - toposort=1.10 - python>=3.10 - pip=23.1.2 # bioconda channel installs - fastqc - trimmomatic=0.36 - multiqc - qualimap - pip: - snakemake-executor-plugin-cluster-generic","title":"Running Snakemake with conda"},{"location":"ibu-conda/#notes_on_conda_versions_on_ibu","text":"As of 2024, conda environments generated with module load anaconda3/2022.05 from IBU loads Python version 3.9.12 and conda 4.12.0. Once you activate an environment with conda activate env_name or source activate env_name , the default conda usually get re-directed to the conda that were originally used to create the environment. This is good because it helps ensure that all packages in the same environment uses the same version of conda. One can go to cd ~/.conda/env_name/bin and readlink -f conda or readlink -f activate to see which version of conda is used by this environment. This is exactly how Snakemake determines which conda to use when using an existing conda environment.","title":"Notes on conda versions on IBU"},{"location":"ibu-conda/#a_faster_alternative_to_conda","text":"In recent years, a 'drop-in' replacement for Conda, called Mamba, was released. Mamba is faster at resolving dependencies, which improves wait times for creating and activating environments. Conda and Mamba seem to be forward and backwards compatible, and they share the same command structure. For example, conda create and mamba create do the same task and share the same parameters. If you prefer to use Mamba, you can install it in your home directory by running these commands: cd ~ wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh chmod +x ./Mambaforge-Linux-x86_64.sh ./Mambaforge-Linux-x86_64.sh After completing the installation, the mamba binary should be added to $PATH , and you should be able to create, configure, and activate environments using the same commands as conda (FYI the conda binary will still be available to be called from $PATH by default). You will immediately notice a difference in speed when using mamba. Test it by loading an environment: mamba activate /data/users/<user_id>/conda_envs/<env_name>","title":"A faster alternative to Conda"},{"location":"ibu-intro/","text":"Introduction \u00b6 Introduction New Users Signing into IBU Login Nodes Home Directory Partitions/Queues Request 'interact' sessions on IBU Using screen or nohup to keep jobs from timing out Using packages already installed on IBU Submitting jobs to IBU Monitoring SLURM Jobs on IBU The Parisod Lab makes use of IBU, the HPC at University of Bern. Take some time to read over the overview of what IBU is, what it does, how to use it, and how to sign up: IBU Documentation Note New to the command line? Check out the quick and dirty bash intro or this introduction first! New Users \u00b6 To gain access to IBU: Register new user by requesting IBU allocation and parisod group access to Pierre. Signing into IBU \u00b6 After you gain access to the cluster you can login using: ssh <user_id>@login8.hpc.binf.unibe.ch To avoid typing in the password everytime, one can set up a ssh key . If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below). Login Nodes \u00b6 When you login you will be assigned to a login node. . Home Directory \u00b6 Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 50 Gb. You can quickly check the storage of your home directory (and any other storage attached to your user) using the following script: lsquota You can also check your storage space using the following command: du -sh * More information is provided below to help install and use software. Partitions/Queues \u00b6 IBU has several partitions/queues where you can submit jobs. They have different computational resources, and are designed to run specific tasks that vary in complexity and runtime: pshort_el8 : This partition is designed to run quick jobs like tests, debugging, or interactive computing (Jupyter notebooks, R-Studio, etc). pibu_el8 : This partition is designed to run a mix of jobs, from sequential jobs needing just one core to parallel jobs. Nodes on this queue may share resources with many other jobs. pgpu : This partition/queue is designed to run jobs that require the use of GPU nodes. The node has 2 Nvidia A100 gpus with 40GB of memory per gpu. phighmem : This queue contains nodes with 2000GB of memory and it is designed to run jobs that require more than 192GB of memory. Use of this queue requires a separate allocation reIBU. Note The Allocation defines the user group access to these partitions. If you are not sure which partition to use, ask your PI or the IBU support team. Note Anyone who uses IBU should build your own project folder under /data/projects/<project_number>/<user_id> with your name. You should only write and revise files under your project folder. You can read/copy data from p910 but don't write any data out of your project folder. See the Storage section for more information. Important It is important that we keep the 120 Tb of storage space on p910 from filling up with extraneous or intermediate files. It is good practice to clean up old data/files and backup important data at least every few months. You can check the percent of space remaining with quotas.py Request 'interact' sessions on IBU \u00b6 If you are running a few simple commands or want to experiment with files directly you can start an interactive session on IBU. The command below (based on the srun command) will give you access to a node where you can run your command: srun -p pibu_el8 -c 4 --mem 4G --time 2-0 --pty /bin/bash Where -c is the number of cores, --mem is the allocated memory, -p is the partition/queue, and --time is the walltime for the session. Important Do not run commands for big data directly on login node, they are not meant for running heavy-load workflows. Either open an interact session, or submit a job. Using screen or nohup to keep jobs from timing out \u00b6 If you have ever tried to run a pipeline or script that takes a long time (think snake_gatk ), you know that if you close down your terminal or if your IBU session logs out, it will cause your jobs to end prematurely. There are several ways to avoid this: screen Perhaps the most common way to deal with scripts that run for a long time is screen . For the most simple case use, type screen to open a new screen session and then run your script like normal. Below are some more intermediate commands for taking full advantage of screen : screen -R <some_descriptive_name> : Use this command to name your screen session. Especially useful if you have several scren sessions running and/or want to get back to this particular one later. Ctrl+a follwed by Ctrl+d to detach from the current screen session (NOT Ctrl+a+d !) exit to end the current screen session screen -ls lists the IDs of all screen sessions currently running screen -R <screen_id> : Use this command to resume a particular screen session. Important When using screen on IBU, take note that screen sessions are only visible/available when you are logged on to the particular node it was created on. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh login02 ). nohup Another way to avoid SIGHUP errors is to use nohup . nohup is very simple to use, simply type nohup before your normal command and that's it! nohup nextflow run main.nf 2>&1 & Running a command with nohup will look a little different than usual because it will not print anything to the console. Instead, it prints all console outputs into a file in the current directory called nohup.out . You can redirect the output to another filename using: nohup nextflow run main.nf > {date}_output.txt 2>&1 & When you exit this window and open a new session, you can always look at the contents of the output file using cat nohup.out to see the progress. Note You can also run nohup in the background to continue using the same window for other processes by running nohup nextflow run main.nf & . By 2>&1 you are redirecting both stdout and stderr to the same file. Using packages already installed on IBU \u00b6 IBU has a collection of packages installed. You can run module avail |& less or module spider to see what packages are currently available on IBU. You can use module load bcftools or module load bcftools/1.15.1 to load packages with the default or a specific version ( module add does the same thing). If you do echo $PATH before and after loading modules, you can see what module does is simply appending paths to the packages into your $PATH so the packages can be found in your environment. module purge will remove all loaded packages and can be helpful to try when troubleshooting. Note You can also load/install software packages into conda environments for use with a specific project/pipeline. Check out the conda tutorial here . Submitting jobs to IBU \u00b6 Jobs on IBU are managed by SLURM . While most of our pipelines are managed by Nextflow, it is useful to know how to submit jobs directly to SLURM. Below is a template to submit an array job to SLURM that will run 10 parallel jobs. One can run it with sbatch script.sh . If you dig into the Nextflow working directory, you can see Nextflow actually generates such scripts and submits them to SLURM on your behalf. Thanks, Nextflow! #!/bin/bash #SBATCH -J name # Name of job #SBATCH -p parallel # Parition/Queue #SBATCH -t 24:00:00 # Walltime/duration of the job (hh:mm:ss) #SBATCH --cpus-per-task=1 # Number of cores (= processors = cpus) for each task #SBATCH --mem-per-cpu=3G # Memory per core needed for a job #SBATCH --array=0-9 # number of parallel jobs to run counting from 0. Make sure to change this to match total number of files. # make a list of all the files you want to process # this script will run a parallel process for each file in this list name_list=(*.bam) # take the nth ($SGE_TASK_ID-th) file in name_list # don't change this line Input=${name_list[$SLURM_ARRAY_TASK_ID]} # then do your operation on this file Output=`echo $Input | sed 's/.bam/.sam/'` Monitoring SLURM Jobs on IBU \u00b6 Here are some commmon commands used to interact with SLURM and check on job status. For more, check out this page . squeue -u <user_id> to check all jobs of a user. scancel <jobid> to cancel a job. scancel {30000..32000} to cancel a range of jobs (job id between 30000 and 32000).","title":"Introduction"},{"location":"ibu-intro/#introduction","text":"Introduction New Users Signing into IBU Login Nodes Home Directory Partitions/Queues Request 'interact' sessions on IBU Using screen or nohup to keep jobs from timing out Using packages already installed on IBU Submitting jobs to IBU Monitoring SLURM Jobs on IBU The Parisod Lab makes use of IBU, the HPC at University of Bern. Take some time to read over the overview of what IBU is, what it does, how to use it, and how to sign up: IBU Documentation Note New to the command line? Check out the quick and dirty bash intro or this introduction first!","title":"Introduction"},{"location":"ibu-intro/#new_users","text":"To gain access to IBU: Register new user by requesting IBU allocation and parisod group access to Pierre.","title":"New Users"},{"location":"ibu-intro/#signing_into_ibu","text":"After you gain access to the cluster you can login using: ssh <user_id>@login8.hpc.binf.unibe.ch To avoid typing in the password everytime, one can set up a ssh key . If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below).","title":"Signing into IBU"},{"location":"ibu-intro/#login_nodes","text":"When you login you will be assigned to a login node. .","title":"Login Nodes"},{"location":"ibu-intro/#home_directory","text":"Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 50 Gb. You can quickly check the storage of your home directory (and any other storage attached to your user) using the following script: lsquota You can also check your storage space using the following command: du -sh * More information is provided below to help install and use software.","title":"Home Directory"},{"location":"ibu-intro/#partitionsqueues","text":"IBU has several partitions/queues where you can submit jobs. They have different computational resources, and are designed to run specific tasks that vary in complexity and runtime: pshort_el8 : This partition is designed to run quick jobs like tests, debugging, or interactive computing (Jupyter notebooks, R-Studio, etc). pibu_el8 : This partition is designed to run a mix of jobs, from sequential jobs needing just one core to parallel jobs. Nodes on this queue may share resources with many other jobs. pgpu : This partition/queue is designed to run jobs that require the use of GPU nodes. The node has 2 Nvidia A100 gpus with 40GB of memory per gpu. phighmem : This queue contains nodes with 2000GB of memory and it is designed to run jobs that require more than 192GB of memory. Use of this queue requires a separate allocation reIBU. Note The Allocation defines the user group access to these partitions. If you are not sure which partition to use, ask your PI or the IBU support team. Note Anyone who uses IBU should build your own project folder under /data/projects/<project_number>/<user_id> with your name. You should only write and revise files under your project folder. You can read/copy data from p910 but don't write any data out of your project folder. See the Storage section for more information. Important It is important that we keep the 120 Tb of storage space on p910 from filling up with extraneous or intermediate files. It is good practice to clean up old data/files and backup important data at least every few months. You can check the percent of space remaining with quotas.py","title":"Partitions/Queues"},{"location":"ibu-intro/#request_interact_sessions_on_ibu","text":"If you are running a few simple commands or want to experiment with files directly you can start an interactive session on IBU. The command below (based on the srun command) will give you access to a node where you can run your command: srun -p pibu_el8 -c 4 --mem 4G --time 2-0 --pty /bin/bash Where -c is the number of cores, --mem is the allocated memory, -p is the partition/queue, and --time is the walltime for the session. Important Do not run commands for big data directly on login node, they are not meant for running heavy-load workflows. Either open an interact session, or submit a job.","title":"Request 'interact' sessions on IBU"},{"location":"ibu-intro/#using_screen_or_nohup_to_keep_jobs_from_timing_out","text":"If you have ever tried to run a pipeline or script that takes a long time (think snake_gatk ), you know that if you close down your terminal or if your IBU session logs out, it will cause your jobs to end prematurely. There are several ways to avoid this: screen Perhaps the most common way to deal with scripts that run for a long time is screen . For the most simple case use, type screen to open a new screen session and then run your script like normal. Below are some more intermediate commands for taking full advantage of screen : screen -R <some_descriptive_name> : Use this command to name your screen session. Especially useful if you have several scren sessions running and/or want to get back to this particular one later. Ctrl+a follwed by Ctrl+d to detach from the current screen session (NOT Ctrl+a+d !) exit to end the current screen session screen -ls lists the IDs of all screen sessions currently running screen -R <screen_id> : Use this command to resume a particular screen session. Important When using screen on IBU, take note that screen sessions are only visible/available when you are logged on to the particular node it was created on. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh login02 ). nohup Another way to avoid SIGHUP errors is to use nohup . nohup is very simple to use, simply type nohup before your normal command and that's it! nohup nextflow run main.nf 2>&1 & Running a command with nohup will look a little different than usual because it will not print anything to the console. Instead, it prints all console outputs into a file in the current directory called nohup.out . You can redirect the output to another filename using: nohup nextflow run main.nf > {date}_output.txt 2>&1 & When you exit this window and open a new session, you can always look at the contents of the output file using cat nohup.out to see the progress. Note You can also run nohup in the background to continue using the same window for other processes by running nohup nextflow run main.nf & . By 2>&1 you are redirecting both stdout and stderr to the same file.","title":"Using screen or nohup to keep jobs from timing out"},{"location":"ibu-intro/#using_packages_already_installed_on_ibu","text":"IBU has a collection of packages installed. You can run module avail |& less or module spider to see what packages are currently available on IBU. You can use module load bcftools or module load bcftools/1.15.1 to load packages with the default or a specific version ( module add does the same thing). If you do echo $PATH before and after loading modules, you can see what module does is simply appending paths to the packages into your $PATH so the packages can be found in your environment. module purge will remove all loaded packages and can be helpful to try when troubleshooting. Note You can also load/install software packages into conda environments for use with a specific project/pipeline. Check out the conda tutorial here .","title":"Using packages already installed on IBU"},{"location":"ibu-intro/#submitting_jobs_to_ibu","text":"Jobs on IBU are managed by SLURM . While most of our pipelines are managed by Nextflow, it is useful to know how to submit jobs directly to SLURM. Below is a template to submit an array job to SLURM that will run 10 parallel jobs. One can run it with sbatch script.sh . If you dig into the Nextflow working directory, you can see Nextflow actually generates such scripts and submits them to SLURM on your behalf. Thanks, Nextflow! #!/bin/bash #SBATCH -J name # Name of job #SBATCH -p parallel # Parition/Queue #SBATCH -t 24:00:00 # Walltime/duration of the job (hh:mm:ss) #SBATCH --cpus-per-task=1 # Number of cores (= processors = cpus) for each task #SBATCH --mem-per-cpu=3G # Memory per core needed for a job #SBATCH --array=0-9 # number of parallel jobs to run counting from 0. Make sure to change this to match total number of files. # make a list of all the files you want to process # this script will run a parallel process for each file in this list name_list=(*.bam) # take the nth ($SGE_TASK_ID-th) file in name_list # don't change this line Input=${name_list[$SLURM_ARRAY_TASK_ID]} # then do your operation on this file Output=`echo $Input | sed 's/.bam/.sam/'`","title":"Submitting jobs to IBU"},{"location":"ibu-intro/#monitoring_slurm_jobs_on_ibu","text":"Here are some commmon commands used to interact with SLURM and check on job status. For more, check out this page . squeue -u <user_id> to check all jobs of a user. scancel <jobid> to cancel a job. scancel {30000..32000} to cancel a range of jobs (job id between 30000 and 32000).","title":"Monitoring SLURM Jobs on IBU"},{"location":"ibu-nextflow/","text":"Nextflow \u00b6 Nextflow Installation Nextflow versions Quest cluster configuration Global Configuration: ~/.nextflow/config Running Nextflow Running Nextflow from a remote directory Running a different branch of a pipeline Running a specific commit Troubleshooting remote pipelines Running Nextflow from a local directory Resume Getting an email or text when complete Writing Nextflow Pipelines Nextflow is an awesome program that allows you to write a computational pipeline by making it simpler to put together many different tasks, maybe even using different programming languages. Nextflow makes parallelism easy and works with SLURM to schedule jobs so you don't have to! Nextflow also supports docker containers and conda enviornments to streamline reproducible pipelines and can be easily adapted to run on different systems - including Google Cloud! Not convinced? Check out this intro . Installation \u00b6 Nextflow can be installed with two easy steps: # download with wget or curl wget -qO- https://get.nextflow.io | bash # or # curl -s https://get.nextflow.io | bash # make binary executable on your system chmod +x nextflow # Move nextflow to directory accessible by your $PATH to avoid having to type full path to nextflow each time # you can check your $PATH with: echo $PATH # it likely contains `/usr/local/bin` so you could move nextflow there mv nextflow /usr/local/bin/ Nextflow versions \u00b6 You might also want to update nextflow or be able to run different versions. This can be done in several different ways Update Nextflow nextflow self-update Use a specific version of Nextflow NXF_VER=20.04.0 nextflow run main.nf Use the nf20 conda environment built for AndersenLab pipelines module load anaconda3/2022.05 source activate /home/<jheid>/data_eande106/software/conda_envs/nf20_env Important If you run the nf20 conda environment, please do not try to update nextflow, this will cause permission denied errors for other lab members . You can deactivate the conda environment with source deactivate (You can check your Nextflow version with nextflow -v or nextflow --version ) Note If you load this conda environment, it is not necessary to have Nextflow version 20 installed on your system -- you don't even need to have Nextflow installed at all! Because different versions might have updates that affect the running of Nextflow, it is important to keep track of the version of Nextflow you are using, as well as all software packages. Important Many of the Andersen Lab pipelines (if not all) are written with the new DSL2 (see below) which requires Nextflow-v20.0+. Loading the nf20 conda environment is a great way to run these pipelines Quest cluster configuration \u00b6 Configuration files allow you to define the way a pipeline is executed on Quest. Read the quest documentation on configuration files Configuration files are defined at a global level in ~/.nextflow/config and on a per-pipeline basis within <pipeline_directory>/nextflow.config . Settings written in <pipeline_directory>/nextflow.config override settings written in ~/.nextflow/config . Global Configuration: ~/.nextflow/config \u00b6 In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project b1042 . Once you have access you will need to modify your global configuration. Set your ~/.nextflow/config file to be the following: process { executor = 'slurm' queue = 'parallel' clusterOptions = '-A eande106 -t 24:00:00 -e errlog.txt' } #The scratch space directory where workDir and tmpDir will be directed to is currently a work in progress# workDir = \"/path/to/scratch/storage/work/<your name>/\" tmpDir = \"/path/to/scratch/storage/tmp\" This configuration file does the following: executor - Sets the executor to slurm scheduler queue - Sets the queue/partition to parallel which submits jobs to IBU nodes. clusterOptions - Sets the account/allocation to eande106 ; granting access to parallel and other queues. workDir - Sets the working directory to scratch space. To better organization, Please build your own folder under [TBD] , and define it here. tmpDir - Creates a temporary working directory. This can be used within workflows when necessary. Running Nextflow \u00b6 Theoretically, running a Nextflow pipeline should be very straightforward (although with any developing pipelines there are always bugs to work out). Running Nextflow from a remote directory \u00b6 The prefered (and sometimes easiest) way to run a nextflow pipeline can be done in just one single step. When this works (see troubleshooting section below), pipelines can be run without first cloning the git repo. You can just tell Nextflow which git repo to use and it will do the rest! This can be helpful to reduce clutter and avoid making changes to the actual pipeline. Additionally, this method allows you to control which branch and/or commit of the pipeline to run, and Nextflow will automatically track that information for you allowing for great reproducibility. Note There is no need to clone a copy of the repo to run pipelines this way. Behind the scenes, nextflow is actually cloning the repo to your home directory and keeping track of branches/commits. # example command to run the latest version of NemaScan nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 # Note: can also write in one line, the \\ were used to make the code more readable: nextflow run andersenlab/nemascan --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv --vcf 20210121 Note Parameters or arguments to Nextflow scripts are designated with the -- . In the case above, there is a parameter called \"in\" which we are setting to be the test.tsv file. When Nextflow is running, it will print to the console the name of each process in the pipeline as well as update on the progress of the script: For example, in the above screenshot from a NemaScan run, there are 14 different processes in the pipeline. Notice that the fix_strain_names_bulk process has already completed! Meanwhile, the vcf_t_geno_matrix process is still running. You can also see that the prepare_gcta_files is actually run 4 times (in this case, because it is run once per 4 traits in my dataset). Another important piece of information from this Nextflow run is the hash that designates the working directory of each process. the [ad/eea615] next to the fix_strain_names_bulk process indicates that the working directory is located at path_to_nextflow_working_directory/ad/eea615... . This can be helpful if you want to go into that directory to see what is actually happening or trouble shoot errors. Note I highly recommend adding this function to your ~/.bash_profile to easily access the Nexflow working directory: gw() {cd /path/to/scratch/space/work/<your_name>/$1*} so that when you type gw 3f/6a21a5 (the hash Nextflow shows that indicates the specific working directory for a process) you will go to that folder automatically. Running a different branch of a pipeline \u00b6 Instead of cloning the repo and then remembering to switch branches, you can specify the branch in your nextflow call. This is most beneficial because the pipeline will then keep a record of which branch you used so future you doesn't have to remember. # the following command runs the \"develop\" branch of nemascan # (not recommended unless you know what you are doing, might break) nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -r develop Running a specific commit \u00b6 Sometimes we want to keep the version of a pipeline the same across several different runs (maybe preparing for a manuscript etc.). To do this, you can again use the -r argument and just provide the commit ID which is a long alphanumeric code that can be found on github or in the log of your previous nextflow run. # the following command runs will always run the exact same code, no matter how many changes to nemascan there are in the future nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -r 769a2b75545d7f870a880447dd31482cfc628792 Troubleshooting remote pipelines \u00b6 For some reason, running nextflow remotely sometimes throws errors and I am not 100% sure why. Here are some of the most common errors I see and how to fix them. 1. It doesn't pull the latest commit Go to github.com and find the latest commit ID and use that with -r XXX (see above on running specific commits) If that doesn't work, you might have to clone the repo and run it locally (see below) 2. It says the \"repository may be corrupt\" I don't understand this error, but I think it happens when you try to run different branches/commits. Usually, I just go to the path where the error says is corrupt and delete the folder with rm -rf . For example: rm -rf /home/<jheid>/.nextflow/assets/andersenlab/nemascan Warning You should always be careful when using rm , especially rm -rf . There is no going back. Make certain you want to delete the folder and everything inside it. In this case, it will be fine because nextflow will just clone it again fresh. Running Nextflow from a local directory \u00b6 Another way to run Nextflow is by first cloning the git repo to your directory and then running the pipeline. This has advantages and disadvantages over running the pipeline remotely (see below), however if you need to make changes to the pipeline specific to your analysis, you will need to follow these steps. git clone https://github.com/AndersenLab/NemaScan.git cd NemaScan nextflow run main.nf --debug Resume \u00b6 Another great thing about Nextflow is that it caches all the processes that completed successfully, so if you have an error at the middle or end of your pipeline, you can re-run the same Nextflow command with -resume and it will start where it finished last time! nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -resume There is usually no downside to adding -resume , so you can get into the habit of always adding it if you want. Important There is some confusion with how the -resume works and sometimes it doesn't work as expected. Check out this and this other guide for more help. One thing I've learned is there is a difference between process.cache = 'deep' vs. 'lenient' . Getting an email or text when complete \u00b6 If you would like to receive an email or text from Nextflow when your pipeline finishes (either successfully or with error), all you need to do is add -N <email> to your code. Most phone companies have a way to \"email\" your phone as an SMS, so you can use this email address to get a text alert. For example: # send email nextflow run andersenlab/nemascan --debug -N kathryn.evans@northwestern.edu # send text to 801-867-5309 with verizon nextflow run andersenlab/nemascan --debut -N 8018675309@vtext.com Writing Nextflow Pipelines \u00b6 See this page for tips on how to get started with your own nextflow pipeline. Also check out the Nextflow documentation for help getting started!","title":"Nextflow"},{"location":"ibu-nextflow/#nextflow","text":"Nextflow Installation Nextflow versions Quest cluster configuration Global Configuration: ~/.nextflow/config Running Nextflow Running Nextflow from a remote directory Running a different branch of a pipeline Running a specific commit Troubleshooting remote pipelines Running Nextflow from a local directory Resume Getting an email or text when complete Writing Nextflow Pipelines Nextflow is an awesome program that allows you to write a computational pipeline by making it simpler to put together many different tasks, maybe even using different programming languages. Nextflow makes parallelism easy and works with SLURM to schedule jobs so you don't have to! Nextflow also supports docker containers and conda enviornments to streamline reproducible pipelines and can be easily adapted to run on different systems - including Google Cloud! Not convinced? Check out this intro .","title":"Nextflow"},{"location":"ibu-nextflow/#installation","text":"Nextflow can be installed with two easy steps: # download with wget or curl wget -qO- https://get.nextflow.io | bash # or # curl -s https://get.nextflow.io | bash # make binary executable on your system chmod +x nextflow # Move nextflow to directory accessible by your $PATH to avoid having to type full path to nextflow each time # you can check your $PATH with: echo $PATH # it likely contains `/usr/local/bin` so you could move nextflow there mv nextflow /usr/local/bin/","title":"Installation"},{"location":"ibu-nextflow/#nextflow_versions","text":"You might also want to update nextflow or be able to run different versions. This can be done in several different ways Update Nextflow nextflow self-update Use a specific version of Nextflow NXF_VER=20.04.0 nextflow run main.nf Use the nf20 conda environment built for AndersenLab pipelines module load anaconda3/2022.05 source activate /home/<jheid>/data_eande106/software/conda_envs/nf20_env Important If you run the nf20 conda environment, please do not try to update nextflow, this will cause permission denied errors for other lab members . You can deactivate the conda environment with source deactivate (You can check your Nextflow version with nextflow -v or nextflow --version ) Note If you load this conda environment, it is not necessary to have Nextflow version 20 installed on your system -- you don't even need to have Nextflow installed at all! Because different versions might have updates that affect the running of Nextflow, it is important to keep track of the version of Nextflow you are using, as well as all software packages. Important Many of the Andersen Lab pipelines (if not all) are written with the new DSL2 (see below) which requires Nextflow-v20.0+. Loading the nf20 conda environment is a great way to run these pipelines","title":"Nextflow versions"},{"location":"ibu-nextflow/#quest_cluster_configuration","text":"Configuration files allow you to define the way a pipeline is executed on Quest. Read the quest documentation on configuration files Configuration files are defined at a global level in ~/.nextflow/config and on a per-pipeline basis within <pipeline_directory>/nextflow.config . Settings written in <pipeline_directory>/nextflow.config override settings written in ~/.nextflow/config .","title":"Quest cluster configuration"},{"location":"ibu-nextflow/#global_configuration_nextflowconfig","text":"In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project b1042 . Once you have access you will need to modify your global configuration. Set your ~/.nextflow/config file to be the following: process { executor = 'slurm' queue = 'parallel' clusterOptions = '-A eande106 -t 24:00:00 -e errlog.txt' } #The scratch space directory where workDir and tmpDir will be directed to is currently a work in progress# workDir = \"/path/to/scratch/storage/work/<your name>/\" tmpDir = \"/path/to/scratch/storage/tmp\" This configuration file does the following: executor - Sets the executor to slurm scheduler queue - Sets the queue/partition to parallel which submits jobs to IBU nodes. clusterOptions - Sets the account/allocation to eande106 ; granting access to parallel and other queues. workDir - Sets the working directory to scratch space. To better organization, Please build your own folder under [TBD] , and define it here. tmpDir - Creates a temporary working directory. This can be used within workflows when necessary.","title":"Global Configuration: ~/.nextflow/config"},{"location":"ibu-nextflow/#running_nextflow","text":"Theoretically, running a Nextflow pipeline should be very straightforward (although with any developing pipelines there are always bugs to work out).","title":"Running Nextflow"},{"location":"ibu-nextflow/#running_nextflow_from_a_remote_directory","text":"The prefered (and sometimes easiest) way to run a nextflow pipeline can be done in just one single step. When this works (see troubleshooting section below), pipelines can be run without first cloning the git repo. You can just tell Nextflow which git repo to use and it will do the rest! This can be helpful to reduce clutter and avoid making changes to the actual pipeline. Additionally, this method allows you to control which branch and/or commit of the pipeline to run, and Nextflow will automatically track that information for you allowing for great reproducibility. Note There is no need to clone a copy of the repo to run pipelines this way. Behind the scenes, nextflow is actually cloning the repo to your home directory and keeping track of branches/commits. # example command to run the latest version of NemaScan nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 # Note: can also write in one line, the \\ were used to make the code more readable: nextflow run andersenlab/nemascan --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv --vcf 20210121 Note Parameters or arguments to Nextflow scripts are designated with the -- . In the case above, there is a parameter called \"in\" which we are setting to be the test.tsv file. When Nextflow is running, it will print to the console the name of each process in the pipeline as well as update on the progress of the script: For example, in the above screenshot from a NemaScan run, there are 14 different processes in the pipeline. Notice that the fix_strain_names_bulk process has already completed! Meanwhile, the vcf_t_geno_matrix process is still running. You can also see that the prepare_gcta_files is actually run 4 times (in this case, because it is run once per 4 traits in my dataset). Another important piece of information from this Nextflow run is the hash that designates the working directory of each process. the [ad/eea615] next to the fix_strain_names_bulk process indicates that the working directory is located at path_to_nextflow_working_directory/ad/eea615... . This can be helpful if you want to go into that directory to see what is actually happening or trouble shoot errors. Note I highly recommend adding this function to your ~/.bash_profile to easily access the Nexflow working directory: gw() {cd /path/to/scratch/space/work/<your_name>/$1*} so that when you type gw 3f/6a21a5 (the hash Nextflow shows that indicates the specific working directory for a process) you will go to that folder automatically.","title":"Running Nextflow from a remote directory"},{"location":"ibu-nextflow/#running_a_different_branch_of_a_pipeline","text":"Instead of cloning the repo and then remembering to switch branches, you can specify the branch in your nextflow call. This is most beneficial because the pipeline will then keep a record of which branch you used so future you doesn't have to remember. # the following command runs the \"develop\" branch of nemascan # (not recommended unless you know what you are doing, might break) nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -r develop","title":"Running a different branch of a pipeline"},{"location":"ibu-nextflow/#running_a_specific_commit","text":"Sometimes we want to keep the version of a pipeline the same across several different runs (maybe preparing for a manuscript etc.). To do this, you can again use the -r argument and just provide the commit ID which is a long alphanumeric code that can be found on github or in the log of your previous nextflow run. # the following command runs will always run the exact same code, no matter how many changes to nemascan there are in the future nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -r 769a2b75545d7f870a880447dd31482cfc628792","title":"Running a specific commit"},{"location":"ibu-nextflow/#troubleshooting_remote_pipelines","text":"For some reason, running nextflow remotely sometimes throws errors and I am not 100% sure why. Here are some of the most common errors I see and how to fix them. 1. It doesn't pull the latest commit Go to github.com and find the latest commit ID and use that with -r XXX (see above on running specific commits) If that doesn't work, you might have to clone the repo and run it locally (see below) 2. It says the \"repository may be corrupt\" I don't understand this error, but I think it happens when you try to run different branches/commits. Usually, I just go to the path where the error says is corrupt and delete the folder with rm -rf . For example: rm -rf /home/<jheid>/.nextflow/assets/andersenlab/nemascan Warning You should always be careful when using rm , especially rm -rf . There is no going back. Make certain you want to delete the folder and everything inside it. In this case, it will be fine because nextflow will just clone it again fresh.","title":"Troubleshooting remote pipelines"},{"location":"ibu-nextflow/#running_nextflow_from_a_local_directory","text":"Another way to run Nextflow is by first cloning the git repo to your directory and then running the pipeline. This has advantages and disadvantages over running the pipeline remotely (see below), however if you need to make changes to the pipeline specific to your analysis, you will need to follow these steps. git clone https://github.com/AndersenLab/NemaScan.git cd NemaScan nextflow run main.nf --debug","title":"Running Nextflow from a local directory"},{"location":"ibu-nextflow/#resume","text":"Another great thing about Nextflow is that it caches all the processes that completed successfully, so if you have an error at the middle or end of your pipeline, you can re-run the same Nextflow command with -resume and it will start where it finished last time! nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -resume There is usually no downside to adding -resume , so you can get into the habit of always adding it if you want. Important There is some confusion with how the -resume works and sometimes it doesn't work as expected. Check out this and this other guide for more help. One thing I've learned is there is a difference between process.cache = 'deep' vs. 'lenient' .","title":"Resume"},{"location":"ibu-nextflow/#getting_an_email_or_text_when_complete","text":"If you would like to receive an email or text from Nextflow when your pipeline finishes (either successfully or with error), all you need to do is add -N <email> to your code. Most phone companies have a way to \"email\" your phone as an SMS, so you can use this email address to get a text alert. For example: # send email nextflow run andersenlab/nemascan --debug -N kathryn.evans@northwestern.edu # send text to 801-867-5309 with verizon nextflow run andersenlab/nemascan --debut -N 8018675309@vtext.com","title":"Getting an email or text when complete"},{"location":"ibu-nextflow/#writing_nextflow_pipelines","text":"See this page for tips on how to get started with your own nextflow pipeline. Also check out the Nextflow documentation for help getting started!","title":"Writing Nextflow Pipelines"},{"location":"intro/","text":"Information for all members of the Parisod lab \u00b6 Important Address: Department of Biology, University of Fribourg PER23-Ecology & Evolution building, Chemin du Mus\u00e9e 15 PI : Prof. Christian Parisod, PER23-102, 0 26 300 88 52, christian.parisod@unifr.ch Administrative assistant: Eirini Maikanti, PER23-001, 026 300 88 50, eirini.maikanti@unifr.ch Tech assistants: Dr. Boris Egger (PER05-0.340G, boris.egger@unifr.ch ) Alain Werro (PER04-01.117, 026 300 88 12) Gardner: Alain M\u00fcller( alain.mueller@unifr.ch ), Beno\u00eet Cl\u00e9ment (Alpine Plants, benoit.clement@unifr.ch ) All HR-related matter as well as room authorizations must be coordinated with the administrative assistant. An access to my.unifr.ch through your campus account will simplify many administrative tasks. Also see https://www.unifr.ch/campus/en/personal-support/administrative.html Our lab deals with molecular, computational and living resources. It involves your contribution by keeping things updated and through lab/greenhouse duties planned in advance. Christian will introduce you to the necessary infrastructures for your project and our elabFTW database for the storage of data and procedures (see \u00a73.6ff). Start your stay by being known in our system and continue it with zealous updates. We use lab meetings to promote general coordination in the lab. A google-calendar ( labparisod@gmail.com ) and Slack (Parisod Lab) is used to make a note about your vacation plans ahead of time, as to organize replacement if necessary. If you must unexpectedly take day(s) off (e.g. illness or unanticipated circumstances), notify Christian directly by e-mail. E-mails to all lab members can be sent through biol-parisodgp@unifr.ch . Two series of scientific seminars are taking place during semesters: Departmental seminars with mostly external guests every Tuesdays (11.15 am, PER04-0.110) and Environmental Biology seminars by internal collaborators every Thursdays (15.15 pm, PER04-0.110). Your attendance and participation are welcome! Register to to such and other teaching-related activities and get information through https://moodle.unifr.ch/ Information for all members of the Parisod lab 1. Lab member expectations & responsibilities 1.1 Everyone (full-time, part-time, and undergraduate internship members) 1.2 PI (christian.parisod@unifr.ch) 1.3 Postdocs 1.4 PhD candidates 1.5 MSc students 1.6 Lab managers 1.7 Undergraduate students and research assistants (s.l.) 2. Code of conduct 2.1 Harassment 2.2 Reproducible research 2.3 Responsible research 3. Organization of the lab 3.1 Access to infrastructure 3.2 Working hours 3.3 PI availability 3.4 Lab meetings 3.5 Lab travels 3.6 Lab archives & elabFTW 3.7 Lab rules 1. Lab member expectations & responsibilities \u00b6 1.1 Everyone (full-time, part-time, and undergraduate internship members) \u00b6 Keep your lab and office space tidy on a daily basis. Take your share for team-level activities. Do work that you are proud of. Do work that others will care about. Double-check your work (good science builds on solid evidence). Would you feel your work does not meet these standards, let\u2019s discuss possible solutions. Science is a marathon. Academia has a specific flavor, but it is still a job. Be professional. Do not come into the lab if you are sick. Take personal time and vacation when needed and cultivate a life outside of the lab. There may be times that you have to work longer or harder to finish something, but you should feel it balanced out over time. We are a team. Be supportive of your lab mates. Attend lab meetings and group activities. Respect each others' strengths, weaknesses, differences... Communicate openly. Work independently when you can, but make the best of working next to peers and ask for help when you need it. Share your knowledge. Typical workflow in the face of a question: (i) consult documentation, (ii) ask google, (iii) ask peers, (iv) ask direct mentor, (v) ask PI. It can be shortcut. We work within an academic system that is fundamentally self-driven. Participate to it with curiosity. Among the several educational events that target a broad audience, regular series of seminars shall normally be attended. An overview is to be found on moodle (register as participant) and in a weekly newsletter. If you have an issue with another lab member that cannot be solved through direct communication, please talk with Christian. If you have an issue with Christian, please reach out the Mediation Service of UNIFR: https://www.unifr.ch/uni/en/administration/mediation-office.html Before quitting the lab for new adventures, sort and archive your documents appropriately, clean your lab and office space, make sure that your samples are properly stored (labelled with your name and a date!) and referenced in our database. Check with your direct supervisor. See \u00a7 4 for further details. 1.2 PI ( christian.parisod@unifr.ch ) \u00b6 All of the above, plus you can expect me to: Maintain a vision of where the lab is going. Apply for and secure the funding necessary to keep the lab going. Sketch out the strategic plan on a yearly basis to keep you on track with your goals. Be available (also see \u00a73.3) to meet with you as regularly as needed to discuss your research and make it progress towards publication. Work with you to regularly develop a mentoring and research plan tailored to your interests, needs, and goals. We will set the definition of \"regularly\" together across your project. Support your career development by introducing you to networks, writing recommendation letters for you, providing you with opportunities to attend conferences and promoting your work in talks and papers. We will have a yearly meeting to focus on professional achievements and perspectives. I am happy to discuss any concern that may be influencing your work. We can always discuss extra support related to time management and productivity and brainstorm possible solutions. 1.3 Postdocs \u00b6 All of the above, plus you will be expected to: Generally develop your own independent line of research and mentor early researchers on their projects, when asked or when appropriate. Apply for external funding. Apply for jobs (academic or industry) as soon as you are \"ready\" and/or at least six months before the term of your contract. We can discuss ways of making sure that you are getting the training you need, while still doing excellent research. I encourage you to seek out opportunities to present your research to the department, research community, or general public. If you are going to give a presentation (including posters and talks), please request a practice presentation to the lab at least one week ahead of time. 1.4 PhD candidates \u00b6 All of the above, plus you will be expected to: Develop a line of dissertation research. Ideally, your dissertation research will consist of at least three chapters (i.e. independent studies/experiments) that can be packaged into one thesis document. Develop your skills (scientific, technical, transferable). Work with undergraduate students, as to not only promote data collection but also experience team managing and mentoring. Apply for external funding, as a valuable learning experience. Question the type of career you want to pursue (e.g. research-focused or teaching-focused, non-academic jobs like data science or science writing). We can brainstorm ways of making sure you are getting the training that you need. Apply for jobs at least six months before the term of your contract. Prioritize time for research (at the end of the training period, you need to have a completed dissertation). Stay up-to-date on any deadlines that you need to meet to fulfill departmental requirements (e.g. funding applications, master\u2019s defense, teaching, \u2026 ). 1.5 MSc students \u00b6 All of the above, plus you will be expected to: Develop a line of dissertation research. Ideally, your dissertation will consist of complementary analyses using data that you have produced and that can ideally be packaged into a scientific publication. Develop your organization and skills (scientific, technical, transferable). Question the type of career you want to pursue (e.g. research-focused or teaching-focused, non-academic jobs like data science or science writing). 1.6 Lab managers \u00b6 All of the above, plus you will be expected to: Maintain the list of lab members updated (including the biol-parisodgp mailing list, elabWTF, slack et al, and access to lab infrastructures through magnetic locks by informing Eirini with name and duration, ...) Maintain the lab internal database and routines updated, with proper archiving of samples and protocols. Order main consumables. Keep this manual up to date. Help to maintain an atmosphere of professionalism within the lab. Oversee the scheduling and training of newcomers in the lab. Ensures proper reporting before the leaving of a lab member. Assist other lab members with data collection, storage or analysis. Work on your own research project. 1.7 Undergraduate students and research assistants (s.l.) \u00b6 Undergraduate research assistants can earn credits or money for their contribution to our lab routines. Enquiries can be made by writing an e-mail to Christian or approaching a lab member. If you are hired as a research assistant (i.e. contract, for typically 4-8 weekly hours or specified time blocks), you will be under the daily supervision of a direct mentor with whom you will determine your schedule and all of the above concerns you. 2. Code of conduct \u00b6 In addition to the general expectations laid out above, I am dedicated to making our lab a safe, inclusive, and welcoming environment for all. 2.1 Harassment \u00b6 All members of the lab, along with visitors, are expected to agree with a decent code of conduct. I expect all lab members to treat one another with respect and to be sensitive to how one\u2019s words and actions impact others. We also know that science benefits from diverse perspectives. Members asked to stop any harassing behavior are expected to comply immediately. \u201cQui vient trop pr\u00e8s va trop loin. Wer zu nah kommt, geht zu weit\u00bb. If you are being harassed, notice that someone else is being harassed, or have any other concerns, please contact Christian or the UNIFR mediation department. 2.2 Reproducible research \u00b6 Reproducible research is research that can be exactly reproduced (i.e. replicability as the ability to generate the same results again). It is expected that all of our research can be justified, at minimum, as reproducible (incl. own testing when possible). That requires to be organized and possess sufficient foresight to document each step of your research process. See \u00a73.6 for our internal database. Scientific integrity https://www.unifr.ch/scimed/en/integrity 2.3 Responsible research \u00b6 Authorship credit should reflect the individual's contribution to the study. An author is considered anyone contributing with initial research design, data collection and analysis, manuscript drafting, and final approval. All authors assume responsibility for the publication, making sure that the data they contributed are accurate. The submitting author makes sure that all deserving authors have been credited, that all authors have given their approval; and handles responses to inquiries during and after publication. Authorship will usually be discussed prior to the beginning of a new project to clearly set expectations. Changes to authorship may occur over the course of a project if new collaborators become involved or if someone is not fulfilling their planned role. In general, I expect that PhD candidates and postdocs in the lab will be first authors on publications on which they are the primary lead, and I will be the last author. As for publications out of the lab, we generally try to avoid thin slices and tend to privilege integrative evidence supporting a complete story as much as possible. Would you feel that our targeted high-hanging fruits can hardly get harvested, let\u2019s discuss how to handle the situation to possibly avoid starvation. For projects that have required significant lab resources, \"ownership\" of data to the producer can be ended on mutual agreement or maximum 2 years after collection. At that point, the PI reserves the right to re-assign the project (or not) as needed to expedite publications. This is to avoid situations of long languishing datasets, while supporting priority. 3. Organization of the lab \u00b6 3.1 Access to infrastructure \u00b6 Lab safety https://www.unifr.ch/scimed/en/safety/commissions Authorization for staff to enable entry through the main entrance of the Ecology & Evolution building (PER23; Chemin du Mus\u00e9e 15) is requested through the administrative assistant. Same for physical keys. When you leave the lab, make sure to return your key (and get any deposit back). 3.2 Working hours \u00b6 One benefit of a career in academic research is typical flexibility. In your contract, 100% legally means being employed for 42 hours a week. You are typically not required to work over-time. I expect you to be in the lab, at minimum, multiple weekdays at peak hours (10am and 2pm or so) to promote interactions. 3.3 PI availability \u00b6 In addition to poking my head into the lab or your office regularly, I will be working on campus and available for meetings most days of the week. My door is semi-open when I can be available. Feel free to pop in so that we can set ad-hoc meetings to shortly discuss. As I may turn my \u201cnotifications\u201d off when concentrating on something else, you can always send me an e-mail. I do my best to shortly answer and will not mind receiving a reminder after more than 48h. If you need something by a particular deadline, please indicate it. Early information (at least, one week beforehand) should enable me to allocate sufficient time and meet your deadline. Recommendation letters: I will usually write a letter for any lab member having spent multiple months in the lab. Your request (ideally, two weeks ahead of time) should include the deadline and your current CV as well as any relevant instruction. To ensure that I do not miss any important details, I may ask you for a draft, which I would then revise consistently. 3.4 Lab meetings \u00b6 Lab meetings, every Tuesday (from 10.15 am, PER23) last 45minutes. If more time is necessary, we will schedule an extraordinary meeting. Every second slot will be focused on project presentations going over new data or methods. Every other slot will be a journal club with the Flatt group. All full-time lab members are expected to attend lab meetings. Part-time internships are welcome. To ensure effective lab meetings, it is recommended to adequately prepare, especially when presenting. This promotes not only your progress, but also fosters useful feedback. This is also a privileged time to set bilateral meetings and foster cooperation. 3.5 Lab travels \u00b6 The lab will typically pay for full-time lab members to work in the lab of collaborators or to present their work at conferences (e.g. National, such as Biology\u20192i; International). In general, presented work should be original and appropriate for the targeted conference. Wishes must be discussed beforehand as they may depend on the availability of funds. Transparent reimbursement of your work-related costs must in all cases be justified by invoices and must be coordinated with administrative assistant based on founding source). Lab members can apply for other sources of funding available to them (e.g. Swiss Botanical Society). 3.6 Lab archives & elabFTW \u00b6 We have three alternatives for data storage: -Common server of Biology, for light data MacOS/linux: smb://common.unifr.ch/biol/_Ecologie/Parisod_group/ Windows: [\\\\common.unifr.ch\\biol\\_Ecologie\\Parisod_group\\](file:///\\common.unifr.ch\\biol_Ecologie\\Parisod_group) You can for instance find environmental conditions of the greenhouse (Greenhouse folder: temperature, light, humidity), lab meeting reports (Lab_meetings folder), posters and talks (Presentations folder), research documents (Projects, Protocols, Publications, Scripts) in Research_doc folder, and teaching documents. Please, think about feeding it with your contributions. -Big data server, for heavy data matching the moto \u201cwrite once read many\u201d MacOS/linux: smb://bigdata.unifr.ch/science/biol/groupe_parisod/ Windows: [\\\\bigdata.unifr.ch\\science\\biol\\groupe_parisod\\](file:///\\bigdata.unifr.ch\\science\\biol\\groupe_parisod) You can find the main folders Archives (to put all data that are currently in a server such as NCBI or EMBL-EBI or data that are not used anymore), Raw_data (to put data that are still being used), Shared (contains protocols, results and software automatically stored from ElabFTW), Users (space to store your own data). -Electronic lab notebook, to document experiments and data stored in the bigdata server https://parisodlab.unifr.ch:8888 use login and password of the UNIFR (LDAP) once registered (ask Patrick Favre for subscription) ElabFTW allows to record your experiments and can be shared (or not) with others. It allows important lab information, such as chemicals, protocols and scripts, to be stored in a database. All contents can be linked to one another (e.g. experiment to databases, experiment to experiments). Parisod:Pop: Samples from natural population (collected in the wild). Each sample included in our collection should be recorded in Parisod Natural with the following fields: collector\u2019s name, date, locality, coordinates, possibly some comments on subsequent fate (e.g. RNA/DNA extraction). Parisod:Experimental: Samples from experimental populations (crossings...). Each cross samples should be recorded in Parisod experimental with fields (collactor name, date, crossing\u2026). 3.7 Lab rules \u00b6 A detailed protocol is to be followed across labs at PER23. It must be read and signed beforehand. Note that the lab 305 is PCR free. Programming of conditions in the LED growth chamber (PER23-104) and growth cabinets (PER23-205a): Patrick Favre. Greenhouses: Keys available PER23-008. Programming of conditions: Alain Werro.","title":"Lab Introduction"},{"location":"intro/#information_for_all_members_of_the_parisod_lab","text":"Important Address: Department of Biology, University of Fribourg PER23-Ecology & Evolution building, Chemin du Mus\u00e9e 15 PI : Prof. Christian Parisod, PER23-102, 0 26 300 88 52, christian.parisod@unifr.ch Administrative assistant: Eirini Maikanti, PER23-001, 026 300 88 50, eirini.maikanti@unifr.ch Tech assistants: Dr. Boris Egger (PER05-0.340G, boris.egger@unifr.ch ) Alain Werro (PER04-01.117, 026 300 88 12) Gardner: Alain M\u00fcller( alain.mueller@unifr.ch ), Beno\u00eet Cl\u00e9ment (Alpine Plants, benoit.clement@unifr.ch ) All HR-related matter as well as room authorizations must be coordinated with the administrative assistant. An access to my.unifr.ch through your campus account will simplify many administrative tasks. Also see https://www.unifr.ch/campus/en/personal-support/administrative.html Our lab deals with molecular, computational and living resources. It involves your contribution by keeping things updated and through lab/greenhouse duties planned in advance. Christian will introduce you to the necessary infrastructures for your project and our elabFTW database for the storage of data and procedures (see \u00a73.6ff). Start your stay by being known in our system and continue it with zealous updates. We use lab meetings to promote general coordination in the lab. A google-calendar ( labparisod@gmail.com ) and Slack (Parisod Lab) is used to make a note about your vacation plans ahead of time, as to organize replacement if necessary. If you must unexpectedly take day(s) off (e.g. illness or unanticipated circumstances), notify Christian directly by e-mail. E-mails to all lab members can be sent through biol-parisodgp@unifr.ch . Two series of scientific seminars are taking place during semesters: Departmental seminars with mostly external guests every Tuesdays (11.15 am, PER04-0.110) and Environmental Biology seminars by internal collaborators every Thursdays (15.15 pm, PER04-0.110). Your attendance and participation are welcome! Register to to such and other teaching-related activities and get information through https://moodle.unifr.ch/ Information for all members of the Parisod lab 1. Lab member expectations & responsibilities 1.1 Everyone (full-time, part-time, and undergraduate internship members) 1.2 PI (christian.parisod@unifr.ch) 1.3 Postdocs 1.4 PhD candidates 1.5 MSc students 1.6 Lab managers 1.7 Undergraduate students and research assistants (s.l.) 2. Code of conduct 2.1 Harassment 2.2 Reproducible research 2.3 Responsible research 3. Organization of the lab 3.1 Access to infrastructure 3.2 Working hours 3.3 PI availability 3.4 Lab meetings 3.5 Lab travels 3.6 Lab archives & elabFTW 3.7 Lab rules","title":"Information for all members of the Parisod lab"},{"location":"intro/#1_lab_member_expectations_responsibilities","text":"","title":"1. Lab member expectations &amp; responsibilities"},{"location":"intro/#11_everyone_full-time_part-time_and_undergraduate_internship_members","text":"Keep your lab and office space tidy on a daily basis. Take your share for team-level activities. Do work that you are proud of. Do work that others will care about. Double-check your work (good science builds on solid evidence). Would you feel your work does not meet these standards, let\u2019s discuss possible solutions. Science is a marathon. Academia has a specific flavor, but it is still a job. Be professional. Do not come into the lab if you are sick. Take personal time and vacation when needed and cultivate a life outside of the lab. There may be times that you have to work longer or harder to finish something, but you should feel it balanced out over time. We are a team. Be supportive of your lab mates. Attend lab meetings and group activities. Respect each others' strengths, weaknesses, differences... Communicate openly. Work independently when you can, but make the best of working next to peers and ask for help when you need it. Share your knowledge. Typical workflow in the face of a question: (i) consult documentation, (ii) ask google, (iii) ask peers, (iv) ask direct mentor, (v) ask PI. It can be shortcut. We work within an academic system that is fundamentally self-driven. Participate to it with curiosity. Among the several educational events that target a broad audience, regular series of seminars shall normally be attended. An overview is to be found on moodle (register as participant) and in a weekly newsletter. If you have an issue with another lab member that cannot be solved through direct communication, please talk with Christian. If you have an issue with Christian, please reach out the Mediation Service of UNIFR: https://www.unifr.ch/uni/en/administration/mediation-office.html Before quitting the lab for new adventures, sort and archive your documents appropriately, clean your lab and office space, make sure that your samples are properly stored (labelled with your name and a date!) and referenced in our database. Check with your direct supervisor. See \u00a7 4 for further details.","title":"1.1 Everyone (full-time, part-time, and undergraduate internship members)"},{"location":"intro/#12_pi_christianparisodunifrch","text":"All of the above, plus you can expect me to: Maintain a vision of where the lab is going. Apply for and secure the funding necessary to keep the lab going. Sketch out the strategic plan on a yearly basis to keep you on track with your goals. Be available (also see \u00a73.3) to meet with you as regularly as needed to discuss your research and make it progress towards publication. Work with you to regularly develop a mentoring and research plan tailored to your interests, needs, and goals. We will set the definition of \"regularly\" together across your project. Support your career development by introducing you to networks, writing recommendation letters for you, providing you with opportunities to attend conferences and promoting your work in talks and papers. We will have a yearly meeting to focus on professional achievements and perspectives. I am happy to discuss any concern that may be influencing your work. We can always discuss extra support related to time management and productivity and brainstorm possible solutions.","title":"1.2 PI (&#99;&#104;&#114;&#105;&#115;&#116;&#105;&#97;&#110;&#46;&#112;&#97;&#114;&#105;&#115;&#111;&#100;&#64;&#117;&#110;&#105;&#102;&#114;&#46;&#99;&#104;)"},{"location":"intro/#13_postdocs","text":"All of the above, plus you will be expected to: Generally develop your own independent line of research and mentor early researchers on their projects, when asked or when appropriate. Apply for external funding. Apply for jobs (academic or industry) as soon as you are \"ready\" and/or at least six months before the term of your contract. We can discuss ways of making sure that you are getting the training you need, while still doing excellent research. I encourage you to seek out opportunities to present your research to the department, research community, or general public. If you are going to give a presentation (including posters and talks), please request a practice presentation to the lab at least one week ahead of time.","title":"1.3 Postdocs"},{"location":"intro/#14_phd_candidates","text":"All of the above, plus you will be expected to: Develop a line of dissertation research. Ideally, your dissertation research will consist of at least three chapters (i.e. independent studies/experiments) that can be packaged into one thesis document. Develop your skills (scientific, technical, transferable). Work with undergraduate students, as to not only promote data collection but also experience team managing and mentoring. Apply for external funding, as a valuable learning experience. Question the type of career you want to pursue (e.g. research-focused or teaching-focused, non-academic jobs like data science or science writing). We can brainstorm ways of making sure you are getting the training that you need. Apply for jobs at least six months before the term of your contract. Prioritize time for research (at the end of the training period, you need to have a completed dissertation). Stay up-to-date on any deadlines that you need to meet to fulfill departmental requirements (e.g. funding applications, master\u2019s defense, teaching, \u2026 ).","title":"1.4 PhD candidates"},{"location":"intro/#15_msc_students","text":"All of the above, plus you will be expected to: Develop a line of dissertation research. Ideally, your dissertation will consist of complementary analyses using data that you have produced and that can ideally be packaged into a scientific publication. Develop your organization and skills (scientific, technical, transferable). Question the type of career you want to pursue (e.g. research-focused or teaching-focused, non-academic jobs like data science or science writing).","title":"1.5 MSc students"},{"location":"intro/#16_lab_managers","text":"All of the above, plus you will be expected to: Maintain the list of lab members updated (including the biol-parisodgp mailing list, elabWTF, slack et al, and access to lab infrastructures through magnetic locks by informing Eirini with name and duration, ...) Maintain the lab internal database and routines updated, with proper archiving of samples and protocols. Order main consumables. Keep this manual up to date. Help to maintain an atmosphere of professionalism within the lab. Oversee the scheduling and training of newcomers in the lab. Ensures proper reporting before the leaving of a lab member. Assist other lab members with data collection, storage or analysis. Work on your own research project.","title":"1.6 Lab managers"},{"location":"intro/#17_undergraduate_students_and_research_assistants_sl","text":"Undergraduate research assistants can earn credits or money for their contribution to our lab routines. Enquiries can be made by writing an e-mail to Christian or approaching a lab member. If you are hired as a research assistant (i.e. contract, for typically 4-8 weekly hours or specified time blocks), you will be under the daily supervision of a direct mentor with whom you will determine your schedule and all of the above concerns you.","title":"1.7 Undergraduate students and research assistants (s.l.)"},{"location":"intro/#2_code_of_conduct","text":"In addition to the general expectations laid out above, I am dedicated to making our lab a safe, inclusive, and welcoming environment for all.","title":"2. Code of conduct"},{"location":"intro/#21_harassment","text":"All members of the lab, along with visitors, are expected to agree with a decent code of conduct. I expect all lab members to treat one another with respect and to be sensitive to how one\u2019s words and actions impact others. We also know that science benefits from diverse perspectives. Members asked to stop any harassing behavior are expected to comply immediately. \u201cQui vient trop pr\u00e8s va trop loin. Wer zu nah kommt, geht zu weit\u00bb. If you are being harassed, notice that someone else is being harassed, or have any other concerns, please contact Christian or the UNIFR mediation department.","title":"2.1 Harassment"},{"location":"intro/#22_reproducible_research","text":"Reproducible research is research that can be exactly reproduced (i.e. replicability as the ability to generate the same results again). It is expected that all of our research can be justified, at minimum, as reproducible (incl. own testing when possible). That requires to be organized and possess sufficient foresight to document each step of your research process. See \u00a73.6 for our internal database. Scientific integrity https://www.unifr.ch/scimed/en/integrity","title":"2.2 Reproducible research"},{"location":"intro/#23_responsible_research","text":"Authorship credit should reflect the individual's contribution to the study. An author is considered anyone contributing with initial research design, data collection and analysis, manuscript drafting, and final approval. All authors assume responsibility for the publication, making sure that the data they contributed are accurate. The submitting author makes sure that all deserving authors have been credited, that all authors have given their approval; and handles responses to inquiries during and after publication. Authorship will usually be discussed prior to the beginning of a new project to clearly set expectations. Changes to authorship may occur over the course of a project if new collaborators become involved or if someone is not fulfilling their planned role. In general, I expect that PhD candidates and postdocs in the lab will be first authors on publications on which they are the primary lead, and I will be the last author. As for publications out of the lab, we generally try to avoid thin slices and tend to privilege integrative evidence supporting a complete story as much as possible. Would you feel that our targeted high-hanging fruits can hardly get harvested, let\u2019s discuss how to handle the situation to possibly avoid starvation. For projects that have required significant lab resources, \"ownership\" of data to the producer can be ended on mutual agreement or maximum 2 years after collection. At that point, the PI reserves the right to re-assign the project (or not) as needed to expedite publications. This is to avoid situations of long languishing datasets, while supporting priority.","title":"2.3 Responsible research"},{"location":"intro/#3_organization_of_the_lab","text":"","title":"3. Organization of the lab"},{"location":"intro/#31_access_to_infrastructure","text":"Lab safety https://www.unifr.ch/scimed/en/safety/commissions Authorization for staff to enable entry through the main entrance of the Ecology & Evolution building (PER23; Chemin du Mus\u00e9e 15) is requested through the administrative assistant. Same for physical keys. When you leave the lab, make sure to return your key (and get any deposit back).","title":"3.1 Access to infrastructure"},{"location":"intro/#32_working_hours","text":"One benefit of a career in academic research is typical flexibility. In your contract, 100% legally means being employed for 42 hours a week. You are typically not required to work over-time. I expect you to be in the lab, at minimum, multiple weekdays at peak hours (10am and 2pm or so) to promote interactions.","title":"3.2 Working hours"},{"location":"intro/#33_pi_availability","text":"In addition to poking my head into the lab or your office regularly, I will be working on campus and available for meetings most days of the week. My door is semi-open when I can be available. Feel free to pop in so that we can set ad-hoc meetings to shortly discuss. As I may turn my \u201cnotifications\u201d off when concentrating on something else, you can always send me an e-mail. I do my best to shortly answer and will not mind receiving a reminder after more than 48h. If you need something by a particular deadline, please indicate it. Early information (at least, one week beforehand) should enable me to allocate sufficient time and meet your deadline. Recommendation letters: I will usually write a letter for any lab member having spent multiple months in the lab. Your request (ideally, two weeks ahead of time) should include the deadline and your current CV as well as any relevant instruction. To ensure that I do not miss any important details, I may ask you for a draft, which I would then revise consistently.","title":"3.3 PI availability"},{"location":"intro/#34_lab_meetings","text":"Lab meetings, every Tuesday (from 10.15 am, PER23) last 45minutes. If more time is necessary, we will schedule an extraordinary meeting. Every second slot will be focused on project presentations going over new data or methods. Every other slot will be a journal club with the Flatt group. All full-time lab members are expected to attend lab meetings. Part-time internships are welcome. To ensure effective lab meetings, it is recommended to adequately prepare, especially when presenting. This promotes not only your progress, but also fosters useful feedback. This is also a privileged time to set bilateral meetings and foster cooperation.","title":"3.4 Lab meetings"},{"location":"intro/#35_lab_travels","text":"The lab will typically pay for full-time lab members to work in the lab of collaborators or to present their work at conferences (e.g. National, such as Biology\u20192i; International). In general, presented work should be original and appropriate for the targeted conference. Wishes must be discussed beforehand as they may depend on the availability of funds. Transparent reimbursement of your work-related costs must in all cases be justified by invoices and must be coordinated with administrative assistant based on founding source). Lab members can apply for other sources of funding available to them (e.g. Swiss Botanical Society).","title":"3.5 Lab travels"},{"location":"intro/#36_lab_archives_elabftw","text":"We have three alternatives for data storage: -Common server of Biology, for light data MacOS/linux: smb://common.unifr.ch/biol/_Ecologie/Parisod_group/ Windows: [\\\\common.unifr.ch\\biol\\_Ecologie\\Parisod_group\\](file:///\\common.unifr.ch\\biol_Ecologie\\Parisod_group) You can for instance find environmental conditions of the greenhouse (Greenhouse folder: temperature, light, humidity), lab meeting reports (Lab_meetings folder), posters and talks (Presentations folder), research documents (Projects, Protocols, Publications, Scripts) in Research_doc folder, and teaching documents. Please, think about feeding it with your contributions. -Big data server, for heavy data matching the moto \u201cwrite once read many\u201d MacOS/linux: smb://bigdata.unifr.ch/science/biol/groupe_parisod/ Windows: [\\\\bigdata.unifr.ch\\science\\biol\\groupe_parisod\\](file:///\\bigdata.unifr.ch\\science\\biol\\groupe_parisod) You can find the main folders Archives (to put all data that are currently in a server such as NCBI or EMBL-EBI or data that are not used anymore), Raw_data (to put data that are still being used), Shared (contains protocols, results and software automatically stored from ElabFTW), Users (space to store your own data). -Electronic lab notebook, to document experiments and data stored in the bigdata server https://parisodlab.unifr.ch:8888 use login and password of the UNIFR (LDAP) once registered (ask Patrick Favre for subscription) ElabFTW allows to record your experiments and can be shared (or not) with others. It allows important lab information, such as chemicals, protocols and scripts, to be stored in a database. All contents can be linked to one another (e.g. experiment to databases, experiment to experiments). Parisod:Pop: Samples from natural population (collected in the wild). Each sample included in our collection should be recorded in Parisod Natural with the following fields: collector\u2019s name, date, locality, coordinates, possibly some comments on subsequent fate (e.g. RNA/DNA extraction). Parisod:Experimental: Samples from experimental populations (crossings...). Each cross samples should be recorded in Parisod experimental with fields (collactor name, date, crossing\u2026).","title":"3.6 Lab archives &amp; elabFTW"},{"location":"intro/#37_lab_rules","text":"A detailed protocol is to be followed across labs at PER23. It must be read and signed beforehand. Note that the lab 305 is PCR free. Programming of conditions in the LED growth chamber (PER23-104) and growth cabinets (PER23-205a): Patrick Favre. Greenhouses: Keys available PER23-008. Programming of conditions: Alain Werro.","title":"3.7 Lab rules"},{"location":"labsite/","text":"parisodlab.github.io \u00b6 parisodlab.github.io Getting Started Software-Dependencies Cloning the repo Updating the site Parisodlab.github.io Announcements General Announcements Publication Post Lab members Adding new lab members: Set Status to Former Remove lab members Funding Protocols Research Publications Photo Albums Software Getting Started \u00b6 The Parisod Lab website was built using jekyll and runs using the Github Pages service. Software-Dependencies \u00b6 Several software packages are required for editing/maintaining the Parisod Lab site. They can be installed using Homebrew : # For first time user, run these two commands in your terminal to add Homebrew to your PATH: (echo; echo 'eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"') >> $HOME/.bashrc eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\" # Next follow the instructions below to install the required software packages. brew install ruby imagemagick exiftool python ghostscript brew upgrade ruby # If Ruby has not been updated in a while, you may need to do this. sudo gem install jekyll -v 3.6.0 # If you get an error when trying to run pip, try: # brew link --overwrite python pip install metapub pyyaml Ruby - Is used to run jekyll, which is the software that builds the site. Jekyll - As stated earlier, jekyll builds the static site and is written in Ruby. Imagemagick - Handles thumbnail generation and scaling photos. Imagemagick is used in the build.sh script. exiftool Extract data about photos as part of the build.sh script for use in scaling images. Python Retrieves information about publications and updates _data/pubs_data.yaml . Cloning the repo \u00b6 To get started editing, clone the repo: git clone https://github.com/parisodlab/parisodlab.github.io This repo contains documents that get compiled into the Parisod Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below. You can also use Github Desktop to manage changes to the site. If you want to edit the site locally and preview changes, run the following in the root directory of the git repo: jekyll serve The site should become available at localhost:4000 and any changes you make will be reflected at that local url. Updating the site \u00b6 In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be committing these changes to the repo and pushing the commit to GitHub.com. See Git-SCM for a basic introduction to git. Parisodlab.github.io \u00b6 The structure of the Parisod Lab repo looks like this: CNAME LICENSE README.md build.sh index.html _config.yml _data/ _includes/ _layouts/ _posts/ _site/ assets/ feeds/ files/ pages/ people/ publications/ scripts/ protocols/ funding/ The folders prefixed with Announcements \u00b6 Announcements are stored in the _posts folder. Posts are organized into folders by year. There is also a _photo_albums folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication. General Announcements \u00b6 To add a new post create a new text file with the following naming scheme: YYYY-MM-DD-title.md For example: 2017-09-24-A new post.md The contents of the file should correspond to the following structure: --- title: \"The title of the post\" layout: post tags: news published: true --- The post content goes here! The top part surrounded by --- is known as the header and has to define a number of variables: layout: post , tags: news , and published: true should always be set and should not change. The only thing you will change is the title . Set a title, and add content below. Because we used a *.md extension when naming the file, we can use markdown in the post to create headings, links, images, and more. Publication Post \u00b6 New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header: subtitle: - Usually the title of the paper; Appears on homepage. PMID: - The pubmed identifier Example : --- title: \"Sandra's paper accepted at <em>G3</em>!\" subtitle: \"Ice age-driven range shifts of diploids and expanding autotetraploids of <em>Biscutella laevigata</em> within a conserved niche.\" layout: post tags: news published: true PMID: 39253771 --- Congratulations to Sandra for her paper accepted at G3! Lab members \u00b6 Adding new lab members: \u00b6 (1) - Add a photo of the individual to the people/ folder. (2) - Edit the _data/people.yaml file, and add the information about that individual. Each individual should have - at a minimum, the following: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> photo: <filename of the photo located in the people/ directory> Additional fields can also be added: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> pub_names: [\"<an array>\", \"<of possible>\", \"<publication>\", \"<names>\"] photo: <base filename of the photo located in the people/ directory; e.g. 'dan.jpg'> website: <website> description: <a description of research> email: <email> github: <github username> Note pub_names is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page. Set Status to Former \u00b6 Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a former: true line for that individual and a current_status: line indicating what they are up to. For example: - first_name: Manuel pub_names: - Poretti M last_name: Poretti description: My research broadly spans \"neglected disease\" genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths. title: Postdoctoral Researcher, 2021-2024 photo: manuel.jpg former: true github: mporetti email: manuel.poretti@unifr.ch current_status: Postdoc at University of Melbourne -- <a href='https://ch.linkedin.com/in/manuelporetti'>Manuel's Linkedin Profile</a> Remove lab members \u00b6 Remove the persons information from _data/people.yaml ; Optionally delete their photo. Funding \u00b6 Funding is managed using the funding/ folder in the root directory and the data file _data/funding_links.yaml . The funding/ folder has two subfolders: past/ and current/ for past funding and current funding. Rename the logo file to be lowercase and simple. To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the _data/funding_links.yaml file. This file is structured as a set of basename: url pairs: aws: https://aws.amazon.com/ snf: http://www.snf.ch/en/Pages/default.aspx Each acronym above corresponds with an image file in the current/ or past/ folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here. Protocols \u00b6 Protocols are stored in the protocols/ folder and their titles and pdfs are managed in _data/protocols.yaml . To add a new protocol, add the PDF to the protocols/ folder. Then add these lines to the _data/protocols.yaml file: - Name: Title of Protocol file: filename_of_protocol_in_protocols_folder.pdf group: <em>Biscutella</em> Phenotyping methods name - The name of the protocol file - The filename of the protocol within the protocols/ folder. group - The grouping of the protocol; It will be nested under this grouping on the protocols page. - name: Semi-Quantitative Biomass Assay file: SemiQuantitativeBiomassAssay.pdf group: <em>Biscutella</em> Phenotyping Methods - name: <em>Putella</em> Culture</a> file: PutellaCultureprotocol.pdf group: <em>Biscutella</em> Phenotyping Methods To remove a protocol, delete the pdf and remove the corresponding lines. Research \u00b6 The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to /pages/research and you will see a set of files: research.html - This page controls the content at the top of the research page. It's an overview of research in the Parisod lab. You can edit the top portion between the <p>[content]</p> tags freely to modify the top of the research page. research-*.md - These are the individual projects. These files look like this: --- title: Description of the project image: project_image.jpg order: 1 --- Research content goes here. The page includes a header (the items located between --- ) which includes a number of important items. title - the title to display for the research area. image - An image for that research area/project. This is the base name of the image placed in /assets/img/research/ order - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000). Publications \u00b6 Elements used to construct the publications page of the website are stored in two places: _data/pubs_data.yaml - The publications data stores authors, pub date, journal, etc. publications/ - The publications folder for PDFs, thumbnails, and supplementary files. (1) Download a PDF of the publication To download automatically, you would need to installhttps://github.com/billgreenwald/Pubmed-Batch-Download.git Then you can run the script with: python pubmed_batch_download.py -pmf publications/publications_list.txt -out publications/ This will download the PDFs to the publications/ folder. You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See this guide for information on removing pages from a PDF. Save the PDF to /publications/[year][tag] Where tag is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both. (Optional) PMID Known If the PubMed Identifier (PMID) is known for the publication, you can add it to the file publications/publications_list.txt . (2) Run build.sh The build.sh script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the _data/pubs_data.yaml file if a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the _data/pubs_data.yaml file. (3) Edit _data/pubs_data.yaml The publication should now be added either manually or automatically to _data/pubs_data.yaml and should look something like this: - Authors: - \"Gr\\xFCnig S\" - Patsiou T - Parisod C DOI: 10.1111/nph.20103 Journal: New Phytol PMC: null PMID: '39253771' Title: Ice age-driven range shifts of diploids and expanding autotetraploids of Biscutella laevigata within a conserved niche You will need to add a few things: - Add a PDF: line to associate the publication with the correct PDF and its thumbnail. This is the same tag you used above. - If there is no Date_Published: line you will want to add that. The format is YYYY-month_abbr-DD (e.g. 2017 Aug 17 ). - Add <em> tags around items you want to italicize: <em>Biscutella laevigata</em> Final result: - Authors: - \"Gr\\xFCnig S\" - Patsiou T - Parisod C DOI: 10.1111/nph.20103 Date_Published: 2024 September 16 Journal: New Phytol PMC: null PMID: '39253771' Title: Ice age-driven range shifts of diploids and expanding autotetraploids of <em>Biscutella laevigata</em> within a conserved niche PDF: 2024BiscutellaNewPhytol.pdf (4) Add supplementary data Supplemental data and figures are stored in publications/[pdf_name] . For example, 2024BiscutellaNewPhytol has an associated folder in publications/ where supplemental data and figures are stored: publications/2024BiscutellaNewPhytol/<supplemental files> Once you have added supplemental files, you'll need to add some information to _data/pubs_data.yaml to describe them. These are the lines that were added for 2024BiscutellaNewPhytol : pub_data: files: Supplemental_Figures.pdf: {title: Supplemental Figures} Supplemental_Files.zip: {title: Supplemental Files} Supplemental_Tables.zip: {title: Supplemental Tables}\\ ... <name of file>: {title: <title to display>} The last line above illustrates the format. The name of the file must match exactly what is in the publications/[pdf_name] folder. Resulting supplemental data will be listed under publications and on the data page. Photo Albums \u00b6 Photo albums can be added to the Parisod Labsite. Adding albums requires two utilities to be installed on your computer: (a) Image Magick (b) exiftool These can easily be installed with homebrew . should have been installed during Setup (above), but if not you can install them using the following: brew install imagemagick brew install exiftool (1) Place images in a folder and name it according to the following schema: YYYY-MM-DD-title For example, 2024-06-05-Manuel-Farewell . (2) Move that folder to /people/albums/ (3) Run the build.sh script in the root of the Parisodlab.github.io repo. The build.sh script will do the following: (a) Construct pages for the album being published. (b) Decrease the size of the images in the album (max width=1200). Note The build.sh script also performs other maintenance-related tasks. It is fine to run this script at anytime. You can run the script using: bash build.sh (4) Add the images using git and push to GitHub You can easily add all images using: git add *.jpg (5) Push changes to github git push Software \u00b6 If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.","title":"Parisod Labsite"},{"location":"labsite/#parisodlabgithubio","text":"parisodlab.github.io Getting Started Software-Dependencies Cloning the repo Updating the site Parisodlab.github.io Announcements General Announcements Publication Post Lab members Adding new lab members: Set Status to Former Remove lab members Funding Protocols Research Publications Photo Albums Software","title":"parisodlab.github.io"},{"location":"labsite/#getting_started","text":"The Parisod Lab website was built using jekyll and runs using the Github Pages service.","title":"Getting Started"},{"location":"labsite/#software-dependencies","text":"Several software packages are required for editing/maintaining the Parisod Lab site. They can be installed using Homebrew : # For first time user, run these two commands in your terminal to add Homebrew to your PATH: (echo; echo 'eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"') >> $HOME/.bashrc eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\" # Next follow the instructions below to install the required software packages. brew install ruby imagemagick exiftool python ghostscript brew upgrade ruby # If Ruby has not been updated in a while, you may need to do this. sudo gem install jekyll -v 3.6.0 # If you get an error when trying to run pip, try: # brew link --overwrite python pip install metapub pyyaml Ruby - Is used to run jekyll, which is the software that builds the site. Jekyll - As stated earlier, jekyll builds the static site and is written in Ruby. Imagemagick - Handles thumbnail generation and scaling photos. Imagemagick is used in the build.sh script. exiftool Extract data about photos as part of the build.sh script for use in scaling images. Python Retrieves information about publications and updates _data/pubs_data.yaml .","title":"Software-Dependencies"},{"location":"labsite/#cloning_the_repo","text":"To get started editing, clone the repo: git clone https://github.com/parisodlab/parisodlab.github.io This repo contains documents that get compiled into the Parisod Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below. You can also use Github Desktop to manage changes to the site. If you want to edit the site locally and preview changes, run the following in the root directory of the git repo: jekyll serve The site should become available at localhost:4000 and any changes you make will be reflected at that local url.","title":"Cloning the repo"},{"location":"labsite/#updating_the_site","text":"In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be committing these changes to the repo and pushing the commit to GitHub.com. See Git-SCM for a basic introduction to git.","title":"Updating the site"},{"location":"labsite/#parisodlabgithubio_1","text":"The structure of the Parisod Lab repo looks like this: CNAME LICENSE README.md build.sh index.html _config.yml _data/ _includes/ _layouts/ _posts/ _site/ assets/ feeds/ files/ pages/ people/ publications/ scripts/ protocols/ funding/ The folders prefixed with","title":"Parisodlab.github.io"},{"location":"labsite/#announcements","text":"Announcements are stored in the _posts folder. Posts are organized into folders by year. There is also a _photo_albums folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication.","title":"Announcements"},{"location":"labsite/#general_announcements","text":"To add a new post create a new text file with the following naming scheme: YYYY-MM-DD-title.md For example: 2017-09-24-A new post.md The contents of the file should correspond to the following structure: --- title: \"The title of the post\" layout: post tags: news published: true --- The post content goes here! The top part surrounded by --- is known as the header and has to define a number of variables: layout: post , tags: news , and published: true should always be set and should not change. The only thing you will change is the title . Set a title, and add content below. Because we used a *.md extension when naming the file, we can use markdown in the post to create headings, links, images, and more.","title":"General Announcements"},{"location":"labsite/#publication_post","text":"New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header: subtitle: - Usually the title of the paper; Appears on homepage. PMID: - The pubmed identifier Example : --- title: \"Sandra's paper accepted at <em>G3</em>!\" subtitle: \"Ice age-driven range shifts of diploids and expanding autotetraploids of <em>Biscutella laevigata</em> within a conserved niche.\" layout: post tags: news published: true PMID: 39253771 --- Congratulations to Sandra for her paper accepted at G3!","title":"Publication Post"},{"location":"labsite/#lab_members","text":"","title":"Lab members"},{"location":"labsite/#adding_new_lab_members","text":"(1) - Add a photo of the individual to the people/ folder. (2) - Edit the _data/people.yaml file, and add the information about that individual. Each individual should have - at a minimum, the following: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> photo: <filename of the photo located in the people/ directory> Additional fields can also be added: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> pub_names: [\"<an array>\", \"<of possible>\", \"<publication>\", \"<names>\"] photo: <base filename of the photo located in the people/ directory; e.g. 'dan.jpg'> website: <website> description: <a description of research> email: <email> github: <github username> Note pub_names is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page.","title":"Adding new lab members:"},{"location":"labsite/#set_status_to_former","text":"Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a former: true line for that individual and a current_status: line indicating what they are up to. For example: - first_name: Manuel pub_names: - Poretti M last_name: Poretti description: My research broadly spans \"neglected disease\" genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths. title: Postdoctoral Researcher, 2021-2024 photo: manuel.jpg former: true github: mporetti email: manuel.poretti@unifr.ch current_status: Postdoc at University of Melbourne -- <a href='https://ch.linkedin.com/in/manuelporetti'>Manuel's Linkedin Profile</a>","title":"Set Status to Former"},{"location":"labsite/#remove_lab_members","text":"Remove the persons information from _data/people.yaml ; Optionally delete their photo.","title":"Remove lab members"},{"location":"labsite/#funding","text":"Funding is managed using the funding/ folder in the root directory and the data file _data/funding_links.yaml . The funding/ folder has two subfolders: past/ and current/ for past funding and current funding. Rename the logo file to be lowercase and simple. To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the _data/funding_links.yaml file. This file is structured as a set of basename: url pairs: aws: https://aws.amazon.com/ snf: http://www.snf.ch/en/Pages/default.aspx Each acronym above corresponds with an image file in the current/ or past/ folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here.","title":"Funding"},{"location":"labsite/#protocols","text":"Protocols are stored in the protocols/ folder and their titles and pdfs are managed in _data/protocols.yaml . To add a new protocol, add the PDF to the protocols/ folder. Then add these lines to the _data/protocols.yaml file: - Name: Title of Protocol file: filename_of_protocol_in_protocols_folder.pdf group: <em>Biscutella</em> Phenotyping methods name - The name of the protocol file - The filename of the protocol within the protocols/ folder. group - The grouping of the protocol; It will be nested under this grouping on the protocols page. - name: Semi-Quantitative Biomass Assay file: SemiQuantitativeBiomassAssay.pdf group: <em>Biscutella</em> Phenotyping Methods - name: <em>Putella</em> Culture</a> file: PutellaCultureprotocol.pdf group: <em>Biscutella</em> Phenotyping Methods To remove a protocol, delete the pdf and remove the corresponding lines.","title":"Protocols"},{"location":"labsite/#research","text":"The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to /pages/research and you will see a set of files: research.html - This page controls the content at the top of the research page. It's an overview of research in the Parisod lab. You can edit the top portion between the <p>[content]</p> tags freely to modify the top of the research page. research-*.md - These are the individual projects. These files look like this: --- title: Description of the project image: project_image.jpg order: 1 --- Research content goes here. The page includes a header (the items located between --- ) which includes a number of important items. title - the title to display for the research area. image - An image for that research area/project. This is the base name of the image placed in /assets/img/research/ order - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000).","title":"Research"},{"location":"labsite/#publications","text":"Elements used to construct the publications page of the website are stored in two places: _data/pubs_data.yaml - The publications data stores authors, pub date, journal, etc. publications/ - The publications folder for PDFs, thumbnails, and supplementary files. (1) Download a PDF of the publication To download automatically, you would need to installhttps://github.com/billgreenwald/Pubmed-Batch-Download.git Then you can run the script with: python pubmed_batch_download.py -pmf publications/publications_list.txt -out publications/ This will download the PDFs to the publications/ folder. You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See this guide for information on removing pages from a PDF. Save the PDF to /publications/[year][tag] Where tag is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both. (Optional) PMID Known If the PubMed Identifier (PMID) is known for the publication, you can add it to the file publications/publications_list.txt . (2) Run build.sh The build.sh script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the _data/pubs_data.yaml file if a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the _data/pubs_data.yaml file. (3) Edit _data/pubs_data.yaml The publication should now be added either manually or automatically to _data/pubs_data.yaml and should look something like this: - Authors: - \"Gr\\xFCnig S\" - Patsiou T - Parisod C DOI: 10.1111/nph.20103 Journal: New Phytol PMC: null PMID: '39253771' Title: Ice age-driven range shifts of diploids and expanding autotetraploids of Biscutella laevigata within a conserved niche You will need to add a few things: - Add a PDF: line to associate the publication with the correct PDF and its thumbnail. This is the same tag you used above. - If there is no Date_Published: line you will want to add that. The format is YYYY-month_abbr-DD (e.g. 2017 Aug 17 ). - Add <em> tags around items you want to italicize: <em>Biscutella laevigata</em> Final result: - Authors: - \"Gr\\xFCnig S\" - Patsiou T - Parisod C DOI: 10.1111/nph.20103 Date_Published: 2024 September 16 Journal: New Phytol PMC: null PMID: '39253771' Title: Ice age-driven range shifts of diploids and expanding autotetraploids of <em>Biscutella laevigata</em> within a conserved niche PDF: 2024BiscutellaNewPhytol.pdf (4) Add supplementary data Supplemental data and figures are stored in publications/[pdf_name] . For example, 2024BiscutellaNewPhytol has an associated folder in publications/ where supplemental data and figures are stored: publications/2024BiscutellaNewPhytol/<supplemental files> Once you have added supplemental files, you'll need to add some information to _data/pubs_data.yaml to describe them. These are the lines that were added for 2024BiscutellaNewPhytol : pub_data: files: Supplemental_Figures.pdf: {title: Supplemental Figures} Supplemental_Files.zip: {title: Supplemental Files} Supplemental_Tables.zip: {title: Supplemental Tables}\\ ... <name of file>: {title: <title to display>} The last line above illustrates the format. The name of the file must match exactly what is in the publications/[pdf_name] folder. Resulting supplemental data will be listed under publications and on the data page.","title":"Publications"},{"location":"labsite/#photo_albums","text":"Photo albums can be added to the Parisod Labsite. Adding albums requires two utilities to be installed on your computer: (a) Image Magick (b) exiftool These can easily be installed with homebrew . should have been installed during Setup (above), but if not you can install them using the following: brew install imagemagick brew install exiftool (1) Place images in a folder and name it according to the following schema: YYYY-MM-DD-title For example, 2024-06-05-Manuel-Farewell . (2) Move that folder to /people/albums/ (3) Run the build.sh script in the root of the Parisodlab.github.io repo. The build.sh script will do the following: (a) Construct pages for the album being published. (b) Decrease the size of the images in the album (max width=1200). Note The build.sh script also performs other maintenance-related tasks. It is fine to run this script at anytime. You can run the script using: bash build.sh (4) Add the images using git and push to GitHub You can easily add all images using: git add *.jpg (5) Push changes to github git push","title":"Photo Albums"},{"location":"labsite/#software","text":"If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.","title":"Software"},{"location":"pipeline-mapping/","text":"mapping.rules \u00b6 mapping.rules Pipeline overview Usage Testing on IBU Running on IBU The mapping.rules pipeline performs alignment for paired end fastq sequence data at the sample , and outputs BAMs and related information. Those BAMs can be used for downstream analysis including variant calling , concordance analysis and other analyses. This pipeline contains the rules for a SnakeMake workflow for mapping reads to a reference genome and performing various analyses on the mapped data. The rules in this file include: - end : A rule that specifies the input and output files for generating a multiqc report. - mapping : A rule that maps the reads to the reference genome using BWA and generates a BAM file. - MergeBamLanes : A rule that merges BAM files from different lanes into a single BAM file. - MarkDuplicates : A rule that marks duplicate reads in the BAM file. - CoverageReport : A rule that calculates coverage statistics from the deduplicated BAM file. - genomeCov : A rule that generates a coverage file for the entire genome using the bamcov tool. - deeptoolsCov : A rule that generates coverage statistics using the plotCoverage tool from the deeptools package. - goleftCov : A rule that generates coverage statistics using the goleft tool. - samtoolsStats : A rule that generates various statistics using the samtools package. - multiqcCoverage : A rule that generates a multiqc report from the coverage statistics generated by the other rules. Each rule specifies the input and output files, as well as any parameters and shell commands that need to be executed. Pipeline overview \u00b6 You have checked the QC of the FASTQ data using the readQC pipeline You have trimmed the FASTQ data using the trimming pipeline if the read quality was poor based on the readQC pipeline. OR you have skipped trimming if the read quality was good and there we no remaining adapters. You have filled out the config_main.yaml file with the appropriate information. You have installed base snakemake environment using conda env create -f envs/snake_env.yaml and activated it using conda activate snake_env Usage \u00b6 Testing on IBU \u00b6 This command uses a test dataset snakemake --np -s workflow/mapping.rules --profile slurm 2>&1 | tee logs/log_map.txt Running on IBU \u00b6 You should run this in a screen session. Note: if you are having issues running Snakemake or need reminders, check out the Snakemake page. snakemake -s workflow/mapping.rules --profile slurm 2>&1 | tee logs/log_map.txt","title":"Alignment pipeline"},{"location":"pipeline-mapping/#mappingrules","text":"mapping.rules Pipeline overview Usage Testing on IBU Running on IBU The mapping.rules pipeline performs alignment for paired end fastq sequence data at the sample , and outputs BAMs and related information. Those BAMs can be used for downstream analysis including variant calling , concordance analysis and other analyses. This pipeline contains the rules for a SnakeMake workflow for mapping reads to a reference genome and performing various analyses on the mapped data. The rules in this file include: - end : A rule that specifies the input and output files for generating a multiqc report. - mapping : A rule that maps the reads to the reference genome using BWA and generates a BAM file. - MergeBamLanes : A rule that merges BAM files from different lanes into a single BAM file. - MarkDuplicates : A rule that marks duplicate reads in the BAM file. - CoverageReport : A rule that calculates coverage statistics from the deduplicated BAM file. - genomeCov : A rule that generates a coverage file for the entire genome using the bamcov tool. - deeptoolsCov : A rule that generates coverage statistics using the plotCoverage tool from the deeptools package. - goleftCov : A rule that generates coverage statistics using the goleft tool. - samtoolsStats : A rule that generates various statistics using the samtools package. - multiqcCoverage : A rule that generates a multiqc report from the coverage statistics generated by the other rules. Each rule specifies the input and output files, as well as any parameters and shell commands that need to be executed.","title":"mapping.rules"},{"location":"pipeline-mapping/#pipeline_overview","text":"You have checked the QC of the FASTQ data using the readQC pipeline You have trimmed the FASTQ data using the trimming pipeline if the read quality was poor based on the readQC pipeline. OR you have skipped trimming if the read quality was good and there we no remaining adapters. You have filled out the config_main.yaml file with the appropriate information. You have installed base snakemake environment using conda env create -f envs/snake_env.yaml and activated it using conda activate snake_env","title":"Pipeline overview"},{"location":"pipeline-mapping/#usage","text":"","title":"Usage"},{"location":"pipeline-mapping/#testing_on_ibu","text":"This command uses a test dataset snakemake --np -s workflow/mapping.rules --profile slurm 2>&1 | tee logs/log_map.txt","title":"Testing on IBU"},{"location":"pipeline-mapping/#running_on_ibu","text":"You should run this in a screen session. Note: if you are having issues running Snakemake or need reminders, check out the Snakemake page. snakemake -s workflow/mapping.rules --profile slurm 2>&1 | tee logs/log_map.txt","title":"Running on IBU"},{"location":"pipeline-overview/","text":"Pipeline Overview \u00b6 An overview of the sequencing pipelines is shown below. Note The full protocol (in development) can be found here The main pipeline for a project involves quality control, trimming, mapping, and variant calling using GATK4. Parameters: - config_main.yaml: YAML configuration file containing project details and workflow parameters. Functions: - spend_time(start_time, end_time): Calculates the time spent between two given time points and returns it in the format \"hours:minutes:seconds\". Workflow Steps: 1. Read the configuration file. 2. Check if quality control is required. - If yes, perform quality control using Snakemake and log the running time. - If no, check if trimming is required. - If yes, perform trimming using Snakemake and log the running time. - If no, skip trimming step. 3. Perform mapping using Snakemake and log the running time. 4. Perform variant calling using GATK4 using Snakemake and log the running time.","title":"Overview"},{"location":"pipeline-overview/#pipeline_overview","text":"An overview of the sequencing pipelines is shown below. Note The full protocol (in development) can be found here The main pipeline for a project involves quality control, trimming, mapping, and variant calling using GATK4. Parameters: - config_main.yaml: YAML configuration file containing project details and workflow parameters. Functions: - spend_time(start_time, end_time): Calculates the time spent between two given time points and returns it in the format \"hours:minutes:seconds\". Workflow Steps: 1. Read the configuration file. 2. Check if quality control is required. - If yes, perform quality control using Snakemake and log the running time. - If no, check if trimming is required. - If yes, perform trimming using Snakemake and log the running time. - If no, skip trimming step. 3. Perform mapping using Snakemake and log the running time. 4. Perform variant calling using GATK4 using Snakemake and log the running time.","title":"Pipeline Overview"},{"location":"pipeline-readQC/","text":"trim.rules \u00b6 trim.rules Pipeline overview Usage Testing the pipeline on IBU Running the pipeline on IBU The trim.rules workflow performs FASTQ trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. The pipeline contains the rules for trimming fastq files using Trimmomatic in a Snakemake workflow. The rules in this file perform the following tasks: - Import necessary libraries and modules - Read the configuration file - Define wildcard constraints for prefixes and samples - Define a function to get the fastq files for a given prefix - Define a function to get the trimmed fastq files for a given prefix - Define the 'trim' rule which specifies the input, output, log, benchmark, parameters, and wrapper for the Trimmomatic tool Pipeline overview \u00b6 You have downloaded FASTQ Data to a subdirectory within a data directory. You have filled out the config_main.yaml file with the appropriate information. You have installed base snakemake environment using conda env create -f envs/snake_env.yaml and activated it using conda activate snake_env Usage \u00b6 Testing the pipeline on IBU \u00b6 This command uses a test dataset snakemake -np -s workflow/trim.rules --profile slurm 2>&1 | tee logs/log_trim.txt Running the pipeline on IBU \u00b6 Note: if you are having issues running Snakemake or need reminders, check out the Snakemake page. snakemake -s workflow/trim.rules --profile slurm 2>&1 | tee logs/log_trim.txt","title":"QC pipeline"},{"location":"pipeline-readQC/#trimrules","text":"trim.rules Pipeline overview Usage Testing the pipeline on IBU Running the pipeline on IBU The trim.rules workflow performs FASTQ trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. The pipeline contains the rules for trimming fastq files using Trimmomatic in a Snakemake workflow. The rules in this file perform the following tasks: - Import necessary libraries and modules - Read the configuration file - Define wildcard constraints for prefixes and samples - Define a function to get the fastq files for a given prefix - Define a function to get the trimmed fastq files for a given prefix - Define the 'trim' rule which specifies the input, output, log, benchmark, parameters, and wrapper for the Trimmomatic tool","title":"trim.rules"},{"location":"pipeline-readQC/#pipeline_overview","text":"You have downloaded FASTQ Data to a subdirectory within a data directory. You have filled out the config_main.yaml file with the appropriate information. You have installed base snakemake environment using conda env create -f envs/snake_env.yaml and activated it using conda activate snake_env","title":"Pipeline overview"},{"location":"pipeline-readQC/#usage","text":"","title":"Usage"},{"location":"pipeline-readQC/#testing_the_pipeline_on_ibu","text":"This command uses a test dataset snakemake -np -s workflow/trim.rules --profile slurm 2>&1 | tee logs/log_trim.txt","title":"Testing the pipeline on IBU"},{"location":"pipeline-readQC/#running_the_pipeline_on_ibu","text":"Note: if you are having issues running Snakemake or need reminders, check out the Snakemake page. snakemake -s workflow/trim.rules --profile slurm 2>&1 | tee logs/log_trim.txt","title":"Running the pipeline on IBU"},{"location":"pipeline-trimming/","text":"trim.rules \u00b6 trim.rules Pipeline overview Usage Testing the pipeline on IBU Running the pipeline on IBU The trim.rules workflow performs FASTQ trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. The pipeline contains the rules for trimming fastq files using Trimmomatic in a Snakemake workflow. The rules in this file perform the following tasks: - Import necessary libraries and modules - Read the configuration file - Define wildcard constraints for prefixes and samples - Define a function to get the fastq files for a given prefix - Define a function to get the trimmed fastq files for a given prefix - Define the 'trim' rule which specifies the input, output, log, benchmark, parameters, and wrapper for the Trimmomatic tool Pipeline overview \u00b6 You have downloaded FASTQ Data to a subdirectory within a data directory. You have filled out the config_main.yaml file with the appropriate information. You have installed base snakemake environment using conda env create -f envs/snake_env.yaml and activated it using conda activate snake_env Usage \u00b6 Testing the pipeline on IBU \u00b6 This command uses a test dataset snakemake -np -s workflow/trim.rules --profile slurm 2>&1 | tee logs/log_trim.txt Running the pipeline on IBU \u00b6 You should run this in a screen session. Note: if you are having issues running Snakemake or need reminders, check out the Snakemake page. snakemake -s workflow/trim.rules --profile slurm 2>&1 | tee logs/log_trim.txt","title":"Trim pipeline"},{"location":"pipeline-trimming/#trimrules","text":"trim.rules Pipeline overview Usage Testing the pipeline on IBU Running the pipeline on IBU The trim.rules workflow performs FASTQ trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. The pipeline contains the rules for trimming fastq files using Trimmomatic in a Snakemake workflow. The rules in this file perform the following tasks: - Import necessary libraries and modules - Read the configuration file - Define wildcard constraints for prefixes and samples - Define a function to get the fastq files for a given prefix - Define a function to get the trimmed fastq files for a given prefix - Define the 'trim' rule which specifies the input, output, log, benchmark, parameters, and wrapper for the Trimmomatic tool","title":"trim.rules"},{"location":"pipeline-trimming/#pipeline_overview","text":"You have downloaded FASTQ Data to a subdirectory within a data directory. You have filled out the config_main.yaml file with the appropriate information. You have installed base snakemake environment using conda env create -f envs/snake_env.yaml and activated it using conda activate snake_env","title":"Pipeline overview"},{"location":"pipeline-trimming/#usage","text":"","title":"Usage"},{"location":"pipeline-trimming/#testing_the_pipeline_on_ibu","text":"This command uses a test dataset snakemake -np -s workflow/trim.rules --profile slurm 2>&1 | tee logs/log_trim.txt","title":"Testing the pipeline on IBU"},{"location":"pipeline-trimming/#running_the_pipeline_on_ibu","text":"You should run this in a screen session. Note: if you are having issues running Snakemake or need reminders, check out the Snakemake page. snakemake -s workflow/trim.rules --profile slurm 2>&1 | tee logs/log_trim.txt","title":"Running the pipeline on IBU"},{"location":"r/","text":"R \u00b6 R General R resources Using R on IBU Running Rstudio on IBU General R resources \u00b6 If you are looking for some help getting started with R or taking your R to the next level, check out these useful resources: Swirl - interactive R learning Tidyverse workshop and resources Parisod Lab R Knowledge base & Cheatsheet R-bloggers - tips and tricks Using Rprojects to organize scripts Using workflowR for reproducible work Using Rmarkdown to generate reports Also check out the lab_code slack channel for help/ibuions! Using R on IBU \u00b6 Making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. Fortunately, there are several ways to get around this issue: Docker/singularity container - instead of using conda environments (or maybe in addition to), a docker/singularity image can be generated with R packages installed. The benefit is that singularity images can be used on IBU or locally and minimal set up is required. The downside is that on some occasions there can be errors generating the docker image with incompatible package versions. Library Paths - For several recent pipelines on IBU, we can get around using a docker container for R packages by installing the proper versions of R packages to a shared location ( /projects/b1059/software/R_lib_3.6.0 ). With the following code, any lab member can load the R package version from that location when running the pipeline, causing no errors, even if they don't have the package installed in their local R. # to manually install a package to this specific folder # make sure to first load the correct R version, our pipelines are currently using 3.6.0 module load R/3.6.0 # open R R # install package install.packages(\"tidyverse\", lib = \"/projects/b1059/software/R_lib_3.6.0\") # if you load the package normally, it won't work (unless you also have it installed in your path) library(tidyverse) # won't work # load the package from the specific folder library(tidyverse, lib.loc = \"/projects/b1059/software/R_lib_3.6.0\") # or add the path to your local R library to make for easier loading .libPaths(c(\"/projects/b1059/software/R_lib_3.6.0\", .libPaths() )) library(tidyverse) # works # you can add the following lines to a nextflow script to use these R packages # set a parameter for R library path in case it updates in the future params.R_libpath = \"/projects/b1059/software/R_lib_3.6.0\" # add the .libPaths to the top of the script dynamically (we don't want it statically in case someone wants to use the pipeline outside of ibu) echo \".libPaths(c(\\\\\"${params.R_libpath}\\\\\", .libPaths() ))\" | cat - ${workflow.projectDir}/bin/Script.R > Script.R Rscript --vanilla Script.R Using renv for R Package Management : Renv is a package that allows you to create a project-specific library for R packages. This is useful for ensuring that all users have the same version of R packages. To use renv, you need to install the package and then run renv::init() in your R project. This will create a renv.lock file that lists all the packages you have installed in your project. When you share your project with someone else, they can run renv::restore() to install the same versions of the packages you have. This is a good way to ensure that everyone is using the same versions of R packages. Setting Up renv : Install renv : First, you'll need to install renv if it isn't already installed. R install.packages(\"renv\") Initialize renv : Navigate to your project directory and initialize renv . This will create a project-specific library and an renv.lock file that captures the versions of the packages you are using. R renv::init() This will snapshot the current state of your R environment, saving the versions of all packages currently installed and used in the project. Install Packages : You can install any required packages as usual, and renv will manage them within the project\u2019s library. R renv::install(\"tidyverse\") Snapshot Dependencies : Whenever you make changes to your package dependencies, update the lock file to reflect these changes. R renv::snapshot() The renv.lock file can be committed to your version control system to ensure that anyone else using the project can recreate the exact same environment. Restoring the Environment : When someone else clones your project repository, they can restore the project environment using the renv.lock file. R renv::restore() This will install the exact versions of the packages as specified in the renv.lock file, ensuring consistent environments across different users and systems. Using renv in Scripts and Pipelines : You can add the following lines to your scripts to ensure the renv environment is automatically activated when running the script: R if (!requireNamespace(\"renv\", quietly = TRUE)) install.packages(\"renv\") renv::activate() For Nextflow pipelines, you can include renv::activate() at the beginning of your R scripts to make sure the correct environment is loaded: bash echo \"renv::activate()\" | cat - ${workflow.projectDir}/bin/Script.R > Script.R Rscript --vanilla Script.R Running Rstudio on IBU \u00b6 The IBU Analytics Nodes allow users with active IBU allocations to use RStudio from a web browser. See Research Computing: IBU Analytics Nodes for an overview of the system. Go to the Rstudio browser ( make sure you are connected with VPN if you are off campus ). Log in with your netid and password just like you would on IBU. Note The version of R on the Rstudio browser is currently 4.1.1, which is likely different from the version of R you run on IBU. Therefore, you will need to re-install any packages you want to use in the browser. You can set the working directory with setwd(\"path_to_directory\") and then open and save files and data in Rstudio just like you were using it locally on your computer -- but with data and files on IBU!!","title":"R"},{"location":"r/#r","text":"R General R resources Using R on IBU Running Rstudio on IBU","title":"R"},{"location":"r/#general_r_resources","text":"If you are looking for some help getting started with R or taking your R to the next level, check out these useful resources: Swirl - interactive R learning Tidyverse workshop and resources Parisod Lab R Knowledge base & Cheatsheet R-bloggers - tips and tricks Using Rprojects to organize scripts Using workflowR for reproducible work Using Rmarkdown to generate reports Also check out the lab_code slack channel for help/ibuions!","title":"General R resources"},{"location":"r/#using_r_on_ibu","text":"Making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. Fortunately, there are several ways to get around this issue: Docker/singularity container - instead of using conda environments (or maybe in addition to), a docker/singularity image can be generated with R packages installed. The benefit is that singularity images can be used on IBU or locally and minimal set up is required. The downside is that on some occasions there can be errors generating the docker image with incompatible package versions. Library Paths - For several recent pipelines on IBU, we can get around using a docker container for R packages by installing the proper versions of R packages to a shared location ( /projects/b1059/software/R_lib_3.6.0 ). With the following code, any lab member can load the R package version from that location when running the pipeline, causing no errors, even if they don't have the package installed in their local R. # to manually install a package to this specific folder # make sure to first load the correct R version, our pipelines are currently using 3.6.0 module load R/3.6.0 # open R R # install package install.packages(\"tidyverse\", lib = \"/projects/b1059/software/R_lib_3.6.0\") # if you load the package normally, it won't work (unless you also have it installed in your path) library(tidyverse) # won't work # load the package from the specific folder library(tidyverse, lib.loc = \"/projects/b1059/software/R_lib_3.6.0\") # or add the path to your local R library to make for easier loading .libPaths(c(\"/projects/b1059/software/R_lib_3.6.0\", .libPaths() )) library(tidyverse) # works # you can add the following lines to a nextflow script to use these R packages # set a parameter for R library path in case it updates in the future params.R_libpath = \"/projects/b1059/software/R_lib_3.6.0\" # add the .libPaths to the top of the script dynamically (we don't want it statically in case someone wants to use the pipeline outside of ibu) echo \".libPaths(c(\\\\\"${params.R_libpath}\\\\\", .libPaths() ))\" | cat - ${workflow.projectDir}/bin/Script.R > Script.R Rscript --vanilla Script.R Using renv for R Package Management : Renv is a package that allows you to create a project-specific library for R packages. This is useful for ensuring that all users have the same version of R packages. To use renv, you need to install the package and then run renv::init() in your R project. This will create a renv.lock file that lists all the packages you have installed in your project. When you share your project with someone else, they can run renv::restore() to install the same versions of the packages you have. This is a good way to ensure that everyone is using the same versions of R packages. Setting Up renv : Install renv : First, you'll need to install renv if it isn't already installed. R install.packages(\"renv\") Initialize renv : Navigate to your project directory and initialize renv . This will create a project-specific library and an renv.lock file that captures the versions of the packages you are using. R renv::init() This will snapshot the current state of your R environment, saving the versions of all packages currently installed and used in the project. Install Packages : You can install any required packages as usual, and renv will manage them within the project\u2019s library. R renv::install(\"tidyverse\") Snapshot Dependencies : Whenever you make changes to your package dependencies, update the lock file to reflect these changes. R renv::snapshot() The renv.lock file can be committed to your version control system to ensure that anyone else using the project can recreate the exact same environment. Restoring the Environment : When someone else clones your project repository, they can restore the project environment using the renv.lock file. R renv::restore() This will install the exact versions of the packages as specified in the renv.lock file, ensuring consistent environments across different users and systems. Using renv in Scripts and Pipelines : You can add the following lines to your scripts to ensure the renv environment is automatically activated when running the script: R if (!requireNamespace(\"renv\", quietly = TRUE)) install.packages(\"renv\") renv::activate() For Nextflow pipelines, you can include renv::activate() at the beginning of your R scripts to make sure the correct environment is loaded: bash echo \"renv::activate()\" | cat - ${workflow.projectDir}/bin/Script.R > Script.R Rscript --vanilla Script.R","title":"Using R on IBU"},{"location":"r/#running_rstudio_on_ibu","text":"The IBU Analytics Nodes allow users with active IBU allocations to use RStudio from a web browser. See Research Computing: IBU Analytics Nodes for an overview of the system. Go to the Rstudio browser ( make sure you are connected with VPN if you are off campus ). Log in with your netid and password just like you would on IBU. Note The version of R on the Rstudio browser is currently 4.1.1, which is likely different from the version of R you run on IBU. Therefore, you will need to re-install any packages you want to use in the browser. You can set the working directory with setwd(\"path_to_directory\") and then open and save files and data in Rstudio just like you were using it locally on your computer -- but with data and files on IBU!!","title":"Running Rstudio on IBU"},{"location":"shiny/","text":"R Shiny applications \u00b6 R Shiny applications How to start a new shiny app? Simple example Reactivity Publishing your shiny app to shinyapps.io Shiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions. How to start a new shiny app? \u00b6 If you already know R, getting started in shiny just requires learning a few new concepts and some new functions/syntax. Check out this great tutorial to learn more (great video!)! Getting started You can create a shiny application in Rstudio by clicking File > New File > Shiny Web App . Rstudio will install any packages necessary (like shiny ) and then ask if you want your application to be in one file called app.R or two files: ui.R and server.R . Either way is okay. If you have a large, complex application, it might be easier to split up the UI (user interface) and the server (the meat of the application). Just like with Rmarkdown, a new Rshiny application comes preloaded with some basic code for you to get a look at. To run a shiny application from Rstudio, simply press the green \"run application\" button in the upper right of the script panel. A new window should pop up with the application. You can also choose at this point to \"open in browser\" instead by selecting that button in the upper right of the shiny app. You now have the beginning of a shiny app! You can add new visual elements to your application in the ui.R or in the shiny::ui() function in app.R and you can create the objects, plots, and manipulate data in the server . Here are some of the most commonly used objects (for a full list, check out this page ): ** Inputs You can create an input using one of the following functions in the ui , then use that input in the sever with input$input_id * shiny::radioButtons() * shiny::textInput() * shiny::selectInput() * shiny::sliderInput() * shiny::fileInput() * shiny::actionButton() Outputs You must create an output in the server and then display that output in the ui : | Create ouput in server | Show output in UI | | --- | --- | | shiny::renderPlot() | shiny::plotOutput() | | shiny::renderTable() | shiny::tableOutput() | | shiny::renderUI() | shiny::uiOutput() | | shiny::renderText() | shiny::textOutput() | Simple example \u00b6 library(shiny) # Define UI for application that draws a histogram ui <- shiny::fluidPage( # Application title shiny::titlePanel(\"Old Faithful Geyser Data\"), # Sidebar with a slider input for number of bins shiny::sidebarLayout( shiny::sidebarPanel( shiny::sliderInput(\"bins\", \"Number of bins:\", min = 1, max = 50, value = 30) ), # Show a plot of the generated distribution shiny::mainPanel( shiny::plotOutput(\"distPlot\") ) ) ) # Define server logic required to draw a histogram server <- function(input, output) { output$distPlot <- shiny::renderPlot({ # generate bins based on input$bins from ui.R x <- faithful[, 2] bins <- seq(min(x), max(x), length.out = input$bins + 1) # draw the histogram with the specified number of bins hist(x, breaks = bins, col = 'darkgray', border = 'white') }) } # Run the application shiny::shinyApp(ui = ui, server = server) Reactivity \u00b6 Check out this great overview on reactivity , the cornerstone of shiny applications. It talks about how the inputs are related to outputs and when outputs update in response to inputs. Publishing your shiny app to shinyapps.io \u00b6 After testing your new shiny app in Rstudio, you might be ready to deploy to the web for other people to access! The Andersen Lab has their own shinyapps.io account, so if you are making a lab-related app it is best to use this account (for login details, ask Robyn!) Publishing is simple - press the \"publish\" button in the upper right-hand corner of your running application and follow the prompts to select the right account. Once published, your application will be available at https://parisod-lab.shinyapps.io/{your_app_name}","title":"R Shiny applications"},{"location":"shiny/#r_shiny_applications","text":"R Shiny applications How to start a new shiny app? Simple example Reactivity Publishing your shiny app to shinyapps.io Shiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions.","title":"R Shiny applications"},{"location":"shiny/#how_to_start_a_new_shiny_app","text":"If you already know R, getting started in shiny just requires learning a few new concepts and some new functions/syntax. Check out this great tutorial to learn more (great video!)! Getting started You can create a shiny application in Rstudio by clicking File > New File > Shiny Web App . Rstudio will install any packages necessary (like shiny ) and then ask if you want your application to be in one file called app.R or two files: ui.R and server.R . Either way is okay. If you have a large, complex application, it might be easier to split up the UI (user interface) and the server (the meat of the application). Just like with Rmarkdown, a new Rshiny application comes preloaded with some basic code for you to get a look at. To run a shiny application from Rstudio, simply press the green \"run application\" button in the upper right of the script panel. A new window should pop up with the application. You can also choose at this point to \"open in browser\" instead by selecting that button in the upper right of the shiny app. You now have the beginning of a shiny app! You can add new visual elements to your application in the ui.R or in the shiny::ui() function in app.R and you can create the objects, plots, and manipulate data in the server . Here are some of the most commonly used objects (for a full list, check out this page ): ** Inputs You can create an input using one of the following functions in the ui , then use that input in the sever with input$input_id * shiny::radioButtons() * shiny::textInput() * shiny::selectInput() * shiny::sliderInput() * shiny::fileInput() * shiny::actionButton() Outputs You must create an output in the server and then display that output in the ui : | Create ouput in server | Show output in UI | | --- | --- | | shiny::renderPlot() | shiny::plotOutput() | | shiny::renderTable() | shiny::tableOutput() | | shiny::renderUI() | shiny::uiOutput() | | shiny::renderText() | shiny::textOutput() |","title":"How to start a new shiny app?"},{"location":"shiny/#simple_example","text":"library(shiny) # Define UI for application that draws a histogram ui <- shiny::fluidPage( # Application title shiny::titlePanel(\"Old Faithful Geyser Data\"), # Sidebar with a slider input for number of bins shiny::sidebarLayout( shiny::sidebarPanel( shiny::sliderInput(\"bins\", \"Number of bins:\", min = 1, max = 50, value = 30) ), # Show a plot of the generated distribution shiny::mainPanel( shiny::plotOutput(\"distPlot\") ) ) ) # Define server logic required to draw a histogram server <- function(input, output) { output$distPlot <- shiny::renderPlot({ # generate bins based on input$bins from ui.R x <- faithful[, 2] bins <- seq(min(x), max(x), length.out = input$bins + 1) # draw the histogram with the specified number of bins hist(x, breaks = bins, col = 'darkgray', border = 'white') }) } # Run the application shiny::shinyApp(ui = ui, server = server)","title":"Simple example"},{"location":"shiny/#reactivity","text":"Check out this great overview on reactivity , the cornerstone of shiny applications. It talks about how the inputs are related to outputs and when outputs update in response to inputs.","title":"Reactivity"},{"location":"shiny/#publishing_your_shiny_app_to_shinyappsio","text":"After testing your new shiny app in Rstudio, you might be ready to deploy to the web for other people to access! The Andersen Lab has their own shinyapps.io account, so if you are making a lab-related app it is best to use this account (for login details, ask Robyn!) Publishing is simple - press the \"publish\" button in the upper right-hand corner of your running application and follow the prompts to select the right account. Once published, your application will be available at https://parisod-lab.shinyapps.io/{your_app_name}","title":"Publishing your shiny app to shinyapps.io"},{"location":"sra/","text":"Uploading sequence data to SRA \u00b6 For each dataset, it is important to also upload the FASTQ files to NCBI's Sequence Read Archive (SRA) or ENA (See. ena.md ). If a bioproject already exists, you can create a new submission and link to the previous bioproject . If there is no previous bioproject, you can create a new bioproject and add all relevant data. See below for more instructions. SRA submission \u00b6 Begin submission In the SRA submission portal , click the button for \"New submission\" and follow the prompts. Remember to add the previous bioproject ID if applicable to link this submission to previous submissions. Select \"Model organism or animal\" for biosample type, select \"upload file using excel\" and download the template Create biosample sheet (an example can be found below) An easy starting point here is the sample sheet used for alignment-nf . You will keep the id as the sample name (unique identifier) and strain (note strain also needs to be unique - if there are multiple library preps for the same strain, please append \"-2\" etc. to the strain) To these two columns, you can add bioproject, organism, developmental stage, sex, and tissue (same across all strains) Finally, join this data with the WI species master sheet to get collected by, collection date, and latitude/longitude. - Note: latitude/longitude need to be converted into one shared column in the format \"34.89 S 56.15 W\" (+ refers to North and East) Copy the data into the relevent columns in the template , save, and upload to the submission portal Create SRA metadata sheet Again, select \"upload a file using excel\" and download the template An easy starting point here is, again, the sample sheet used for alignment-nf . You will keep the id as the sample name and the lb as library id. You will also keep fq1 and fq2 for filename and filename2. You will then add the rest of the columns as shown below. Note: the formatting is very specific for this sheet. The title can be found on the bioproject page. The instrument_model can be found by using the \"sequencing_folder\" (not shown, but part of the original sample sheet) and looking up the instrument that folder was run on in the Sequencing Runs google sheet ( here ) Copy and paste the rows from this file into the template to check for correct formatting. Then save the tab as a tsv and upload to the submission portal. Pre-upload FASTQ files using FTP Create a list of files to upload to the FTP server by combining the filename1 and filename2 from the SRA metadata sheet (above). Begin submission by creating an NCBI account (or signing in -- personal account). Then follow the link to the SRA submission portal Follow the instructions under the \"FTP upload\": # establish FTP connection from terminal (on QUEST!) # ftp <address> ftp ftp-private.ncbi.nlm.nih.gov # navigate to your account folder (i.e.) cd uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4 # create new folder for submission (i.e.) mkdir 20210121_submission # exit FTP connection exit # back on quest, run the following line to transfer every file with path listed in \"files_to_upload.tsv\" to that folder # make sure to change your upload folder and files to upload module load parallel parallel --verbose lftp -e \\\"put -O /uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4/20210121_submission {}\\; bye\\\" -u subftp,w4pYB9VQ ftp-private.ncbi.nlm.nih.gov < files_to_upload.tsv Note: it is important that this step is completely finished before you complete your SRA submission Complete submission When finished, in the SRA portal you will be asked to select which folder you want to pull files from Review and submit! If there are any issues they will let you know. List of current bioprojects associated with the Andersen Lab \u00b6 C. elegans WI genome FASTQ - PRJNA549503 (link here )","title":"Uploading sequence data to SRA"},{"location":"sra/#uploading_sequence_data_to_sra","text":"For each dataset, it is important to also upload the FASTQ files to NCBI's Sequence Read Archive (SRA) or ENA (See. ena.md ). If a bioproject already exists, you can create a new submission and link to the previous bioproject . If there is no previous bioproject, you can create a new bioproject and add all relevant data. See below for more instructions.","title":"Uploading sequence data to SRA"},{"location":"sra/#sra_submission","text":"Begin submission In the SRA submission portal , click the button for \"New submission\" and follow the prompts. Remember to add the previous bioproject ID if applicable to link this submission to previous submissions. Select \"Model organism or animal\" for biosample type, select \"upload file using excel\" and download the template Create biosample sheet (an example can be found below) An easy starting point here is the sample sheet used for alignment-nf . You will keep the id as the sample name (unique identifier) and strain (note strain also needs to be unique - if there are multiple library preps for the same strain, please append \"-2\" etc. to the strain) To these two columns, you can add bioproject, organism, developmental stage, sex, and tissue (same across all strains) Finally, join this data with the WI species master sheet to get collected by, collection date, and latitude/longitude. - Note: latitude/longitude need to be converted into one shared column in the format \"34.89 S 56.15 W\" (+ refers to North and East) Copy the data into the relevent columns in the template , save, and upload to the submission portal Create SRA metadata sheet Again, select \"upload a file using excel\" and download the template An easy starting point here is, again, the sample sheet used for alignment-nf . You will keep the id as the sample name and the lb as library id. You will also keep fq1 and fq2 for filename and filename2. You will then add the rest of the columns as shown below. Note: the formatting is very specific for this sheet. The title can be found on the bioproject page. The instrument_model can be found by using the \"sequencing_folder\" (not shown, but part of the original sample sheet) and looking up the instrument that folder was run on in the Sequencing Runs google sheet ( here ) Copy and paste the rows from this file into the template to check for correct formatting. Then save the tab as a tsv and upload to the submission portal. Pre-upload FASTQ files using FTP Create a list of files to upload to the FTP server by combining the filename1 and filename2 from the SRA metadata sheet (above). Begin submission by creating an NCBI account (or signing in -- personal account). Then follow the link to the SRA submission portal Follow the instructions under the \"FTP upload\": # establish FTP connection from terminal (on QUEST!) # ftp <address> ftp ftp-private.ncbi.nlm.nih.gov # navigate to your account folder (i.e.) cd uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4 # create new folder for submission (i.e.) mkdir 20210121_submission # exit FTP connection exit # back on quest, run the following line to transfer every file with path listed in \"files_to_upload.tsv\" to that folder # make sure to change your upload folder and files to upload module load parallel parallel --verbose lftp -e \\\"put -O /uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4/20210121_submission {}\\; bye\\\" -u subftp,w4pYB9VQ ftp-private.ncbi.nlm.nih.gov < files_to_upload.tsv Note: it is important that this step is completely finished before you complete your SRA submission Complete submission When finished, in the SRA portal you will be asked to select which folder you want to pull files from Review and submit! If there are any issues they will let you know.","title":"SRA submission"},{"location":"sra/#list_of_current_bioprojects_associated_with_the_andersen_lab","text":"C. elegans WI genome FASTQ - PRJNA549503 (link here )","title":"List of current bioprojects associated with the Andersen Lab"},{"location":"vscode/","text":"Code in VScode \u00b6 Code in VScode Overview Steps 1. Install the Remote - SSH extension 2. Connect to the remote server 3. Set keybindings to run code on the remote server 4. Passwordless SSH Access in VS Code Overview \u00b6 This guide will walk you through how to connect to a remote server using Visual Studio Code (VScode). This is useful if you want to edit files on a remote server without having to use a terminal or a text editor like vim or nano . You can also setup keybindings to run code on the remote server, which is useful to document your code while you are creating and testing it. Steps \u00b6 1. Install the Remote - SSH extension \u00b6 Open VScode Click on the Extensions icon on the left-hand side of the window Search for Remote - SSH in the search bar Click on the green Install button 2. Connect to the remote server \u00b6 Click on the Remote Explorer icon on the left-hand side of the window (it looks like a little monitor) Click on the + icon at the top of the window Enter the username and IP address/server id of the server you want to connect to (e.g. username@ip_address , e.g. <user_id>@login8.hpc.binf.unibe.ch -IBU or <user_id>@biolpc045604 - p910) Select the SSH configuration file you want to use and update (e.g. ~/.ssh/config ) Once you receive Host added message, click Connect , you will be prompted to enter your password Once connected you need to open a folder on the remote server. Click on the Open Folder button and select the folder you want to open. You will be prompted to enter your password again. You can now Open a terminal on the remote server by clicking on the Terminal menu and selecting New Terminal . Now you can run commands on the remote server, while saving and editing files (e.g. README.md) in VScode. 3. Set keybindings to run code on the remote server \u00b6 Unix/Linux : This step is useful if you want to document your code while you are creating and testing it. 1. Click on the File menu and select Preferences -> Keyboard Shortcuts 2. Search for Terminal: Run Selected Text in Active Terminal and click on the pen icon to add a keybinding 3. Enter the keybinding you want to use (e.g. Ctrl+Enter ) R : Create a dedicated R environment on the remote server using conda to install packages and run R code. install.packages(\"languageserver\") in the above R to enable code completion and syntax highlighting in VScode. Install the R extension REditorSupport.r in VScode to run R code on the remote server. Click on the File menu and select Preferences -> Settings and search for @ext:REditorSupport.r R: Always use Active Terminal and check the box. Follow the steps here: to make vscode mimic RStudio. 4. Passwordless SSH Access in VS Code \u00b6 If you want to avoid entering your password every time you connect to the remote server, you can set up passwordless SSH access. This involves generating SSH keys and configuring your local and remote machines to use them for authentication. 1. Configure Your Local SSH Client Open your SSH configuration file, from vscode. Go to Yiew -> Command Palette -> type Remote-SSH: Open SSH Configuration File and select the file you want to edit. If the file doesn't exist, create it. Add the following configuration for your remote server: plaintext Host <host> HostName <host> User <username> IdentityFile \"path/to/.ssh/keys/<host>_rsa\" Replace the placeholders: - <host> : The remote server's hostname or IP address. - <username> : Your username on the remote server. 2. Generate SSH Keys Generate a public/private key pair: bash ssh-keygen -q -b 2048 -P \"\" -f path/to/.ssh/keys/<host>_rsa -t rsa This creates: - path/to/.ssh/keys/<host>_rsa (private key) - path/to/.ssh/keys/<host>_rsa.pub (public key) Ensure the keys are stored securely: bash chmod 700 path/to/.ssh chmod 600 path/to/.ssh/keys/<host>_rsa 3. Copy the Public Key to the Remote Server You can use any method to transfer the public key to the remote server: Using scp bash scp path/to/.ssh/keys/<host>_rsa.pub <username>@<host>:~/ 4. Configure the Remote Server Log in to the remote server: bash ssh <username>@<host> Set up the public key for SSH authentication: bash mkdir -p ~/.ssh cat ~/host_rsa.pub >> ~/.ssh/authorized_keys chmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys If authorized_keys doesn\u2019t exist: bash mv ~/host_rsa.pub ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys 5. Test SSH Connection On your local machine, test the connection: bash ssh <username>@<host> If configured correctly, you should log in without entering a password. 6. Set Up VS Code Open VS Code and install the Remote - SSH extension from the Extensions Marketplace. Open the Command Palette ( Ctrl+Shift+P or Cmd+Shift+P ) and select: Remote-SSH: Connect to Host... Choose the host configured in path/to/.ssh/config . You should connect to your server without entering a password. Troubleshooting Permission Issues: Ensure that .ssh and key files have the correct permissions: .ssh : 700 Private key: 600 authorized_keys : 600","title":"VScode"},{"location":"vscode/#code_in_vscode","text":"Code in VScode Overview Steps 1. Install the Remote - SSH extension 2. Connect to the remote server 3. Set keybindings to run code on the remote server 4. Passwordless SSH Access in VS Code","title":"Code in VScode"},{"location":"vscode/#overview","text":"This guide will walk you through how to connect to a remote server using Visual Studio Code (VScode). This is useful if you want to edit files on a remote server without having to use a terminal or a text editor like vim or nano . You can also setup keybindings to run code on the remote server, which is useful to document your code while you are creating and testing it.","title":"Overview"},{"location":"vscode/#steps","text":"","title":"Steps"},{"location":"vscode/#1_install_the_remote_-_ssh_extension","text":"Open VScode Click on the Extensions icon on the left-hand side of the window Search for Remote - SSH in the search bar Click on the green Install button","title":"1. Install the Remote - SSH extension"},{"location":"vscode/#2_connect_to_the_remote_server","text":"Click on the Remote Explorer icon on the left-hand side of the window (it looks like a little monitor) Click on the + icon at the top of the window Enter the username and IP address/server id of the server you want to connect to (e.g. username@ip_address , e.g. <user_id>@login8.hpc.binf.unibe.ch -IBU or <user_id>@biolpc045604 - p910) Select the SSH configuration file you want to use and update (e.g. ~/.ssh/config ) Once you receive Host added message, click Connect , you will be prompted to enter your password Once connected you need to open a folder on the remote server. Click on the Open Folder button and select the folder you want to open. You will be prompted to enter your password again. You can now Open a terminal on the remote server by clicking on the Terminal menu and selecting New Terminal . Now you can run commands on the remote server, while saving and editing files (e.g. README.md) in VScode.","title":"2. Connect to the remote server"},{"location":"vscode/#3_set_keybindings_to_run_code_on_the_remote_server","text":"Unix/Linux : This step is useful if you want to document your code while you are creating and testing it. 1. Click on the File menu and select Preferences -> Keyboard Shortcuts 2. Search for Terminal: Run Selected Text in Active Terminal and click on the pen icon to add a keybinding 3. Enter the keybinding you want to use (e.g. Ctrl+Enter ) R : Create a dedicated R environment on the remote server using conda to install packages and run R code. install.packages(\"languageserver\") in the above R to enable code completion and syntax highlighting in VScode. Install the R extension REditorSupport.r in VScode to run R code on the remote server. Click on the File menu and select Preferences -> Settings and search for @ext:REditorSupport.r R: Always use Active Terminal and check the box. Follow the steps here: to make vscode mimic RStudio.","title":"3. Set keybindings to run code on the remote server"},{"location":"vscode/#4_passwordless_ssh_access_in_vs_code","text":"If you want to avoid entering your password every time you connect to the remote server, you can set up passwordless SSH access. This involves generating SSH keys and configuring your local and remote machines to use them for authentication. 1. Configure Your Local SSH Client Open your SSH configuration file, from vscode. Go to Yiew -> Command Palette -> type Remote-SSH: Open SSH Configuration File and select the file you want to edit. If the file doesn't exist, create it. Add the following configuration for your remote server: plaintext Host <host> HostName <host> User <username> IdentityFile \"path/to/.ssh/keys/<host>_rsa\" Replace the placeholders: - <host> : The remote server's hostname or IP address. - <username> : Your username on the remote server. 2. Generate SSH Keys Generate a public/private key pair: bash ssh-keygen -q -b 2048 -P \"\" -f path/to/.ssh/keys/<host>_rsa -t rsa This creates: - path/to/.ssh/keys/<host>_rsa (private key) - path/to/.ssh/keys/<host>_rsa.pub (public key) Ensure the keys are stored securely: bash chmod 700 path/to/.ssh chmod 600 path/to/.ssh/keys/<host>_rsa 3. Copy the Public Key to the Remote Server You can use any method to transfer the public key to the remote server: Using scp bash scp path/to/.ssh/keys/<host>_rsa.pub <username>@<host>:~/ 4. Configure the Remote Server Log in to the remote server: bash ssh <username>@<host> Set up the public key for SSH authentication: bash mkdir -p ~/.ssh cat ~/host_rsa.pub >> ~/.ssh/authorized_keys chmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys If authorized_keys doesn\u2019t exist: bash mv ~/host_rsa.pub ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys 5. Test SSH Connection On your local machine, test the connection: bash ssh <username>@<host> If configured correctly, you should log in without entering a password. 6. Set Up VS Code Open VS Code and install the Remote - SSH extension from the Extensions Marketplace. Open the Command Palette ( Ctrl+Shift+P or Cmd+Shift+P ) and select: Remote-SSH: Connect to Host... Choose the host configured in path/to/.ssh/config . You should connect to your server without entering a password. Troubleshooting Permission Issues: Ensure that .ssh and key files have the correct permissions: .ssh : 700 Private key: 600 authorized_keys : 600","title":"4. Passwordless SSH Access in VS Code"}]}